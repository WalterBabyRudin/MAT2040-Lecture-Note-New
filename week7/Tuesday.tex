\chapterimage{Tuesdayimage}% Chapter heading image

\chapter{Week7}

\section{Tuesday}\index{week7_Tuesday_lecture}\subsection{Quadratic form}
The graphs of the following equations are easy to plot:
\begin{gather}
x^2+y^2=1\label{eq:17.1}\implies\text{ Circle}.\\
\frac{x^2}{2}+\frac{y^2}{5}=1\label{eq:17.2}\implies\text{ Elipse}.\\
\frac{x^2}{2}-\frac{y^2}{5}=1\label{eq:17.3}\implies\text{ Hyperbola}.\\
\left.\begin{align*}
x^2&=\alpha y\\
y^2&=\alpha x
\end{align*}\right\}\implies\text{ Parabola}.
\end{gather}
\begin{figure}[H]
\centering\includegraphics{week7/shape}
\caption{graph for quadratic form equations of two variables}
\end{figure}
The equations $(\ref{eq:17.1})-(\ref{eq:17.3})$ is the \textit{quadratic form equations of two variables}. Now we give the general form for quadratic equations:
\begin{definition}[Quadratic form]
The formula of \emph{quadratic form} is given by
\[
\bm x\trans\bm A\bm x
\]
where $\bm A$ is $n\x n$ matrix, $\bm x\in\mathbb{R}^{n}$.\\
Moreover, sometimes we write $\bm x\trans\bm A\bm x$ as:
\[
\bm x\trans\bm A\bm x=\sum_{i,j=1}^{n}x_ix_ja_{ij}
\]
where $x_i$ is the $i$th entry of $\bm x$ and $a_{ij}$ are $(i,j)$th entry of $\bm A$.
\end{definition}
Moverover, we say an equation is the \emph{conic section of quadratic form} if it can be written as
\[
\bm x\trans\bm A\bm x=1.
\]
\begin{itemize}
\item
Note that $\bm x\trans\bm A\bm x=\bm x\trans\bm A\trans\bm x$. Why?\\ If we take the transpose of $\bm x\trans\bm A\bm x$, since it is a number, so we obtain
\[
(\bm x\trans\bm A\bm x)\trans=\bm x\trans\bm A\bm x.
\]
Since $(\bm x\trans\bm A\bm x)\trans=\bm x\trans\bm A\trans\bm x$, finally we derive 
\[
\bm x\trans\bm A\bm x=\bm x\trans\bm A\trans\bm x.
\]
\item
Hence given any matrix $\bm A$, we always have
\begin{align*}
\bm x\trans\left(\frac{\bm A+\bm A\trans}{2}\right)\bm x&=\frac{1}{2}\bm x\trans\bm A\bm x+\frac{1}{2}\bm x\trans\bm A\trans\bm x\\&=\frac{1}{2}\bm x\trans\bm A\bm x+\frac{1}{2}\bm x\trans\bm A\bm x\\&=\bm x\trans\bm A\bm x.
\end{align*}
Note that $\left(\frac{\bm A+\bm A\trans}{2}\right)$ is \textit{symmetric}! Hence given any $\bm A$, if we want to study its quadratic form, we can always convert this matrix into symmetric matrix.\\
\end{itemize}
Hence without loss of generality, we assume $\bm A=\bm A\trans$ during the section of quadratic form.
\begin{example}\qquad\\
Given the equation $3x^2+2xy+3y^2=1$, how we transform it into the conic section of quadratic form? And how can we determine its shape in view of matirx?\\
Actually, It can be written as
\begin{equation}
\begin{pmatrix}
x&y
\end{pmatrix}\begin{pmatrix}
3&1\\1&3
\end{pmatrix}\begin{pmatrix}
x\\y
\end{pmatrix}=1.\qquad\qquad\text{\emph{conic section of quadratic form.}}
\label{conic_section}
\end{equation}
And we define $\bm A=\begin{pmatrix}
3&1\\1&3
\end{pmatrix}.$ If we do the eigendecomposition for $\bm A$, we obtain
\[
\bm A=\bm Q\Lambda\bm Q\trans
\]
where $\Lambda=\begin{pmatrix}
\lambda_1&\\&\lambda_2
\end{pmatrix}$, $\bm Q=\begin{bmatrix}
\bm x_1&\bm x_2
\end{bmatrix}$. $\bm x_1$,$\bm x_2$ is the eigenvectors of $\bm A$ corresponding to eigenvalues $\lambda_1$,$\lambda_2$ respectively.\\
Thus we convert equation (\ref{conic_section}) into
\[
\begin{pmatrix}
x&y
\end{pmatrix}\bm Q\Lambda\bm Q\trans\begin{pmatrix}
x\\y
\end{pmatrix}=1
\implies
\tilde{\bm x}\trans\Lambda\tilde{\bm x}=1.
\]
where $\tilde{\bm x}=\bm Q\trans\begin{pmatrix}
x\\y
\end{pmatrix}=\begin{pmatrix}
\tilde x_1\\\tilde x_2
\end{pmatrix}$.\\
Then how to determine the shape of this equation? We just do matrix multiplication to obtain:
\[
\lambda_1\tilde x_1^2+\lambda_2\tilde x_2^2=1.
\]
After computation, we find $\lambda_1=4,\lambda_2=2$. Hence this equation is a \emph{elipse.}
\end{example}
\begin{remark}
\subsubsection{Matrix Calculus}
Now we recall how to compute derivative for matrix:
\begin{itemize}
\item
$\frac{\partial(f\trans g)}{\partial x}=\frac{\partial f(x)}{\partial x}g(x)+\frac{\partial g(x)}{\partial x}f(x)$
\end{itemize}
Example:
\begin{itemize}
\item
$\frac{\partial(a\trans \bm x)}{\partial \bm x}=a$
\item
$\frac{\partial(a\trans \bm A\bm x)}{\partial \bm x}=\frac{\partial((\bm A\trans a)\trans\bm x)}{\partial \bm x}=\bm A\trans a$
\item
$\frac{\partial(\bm A\bm x)}{\partial \bm x}=\bm A\trans$
\item
$\frac{\partial(\bm x\trans\bm A\bm x)}{\partial \bm x}=\bm A\bm x+\bm A\trans\bm x$
\end{itemize}
\end{remark}
\begin{example}\qquad\\
Given $f(\bm x)=\frac{1}{2}\bm x\trans\bm A\bm x+\bm b\trans\bm x$. We want to do the optimization:
\[
\min_{\bm x\in\mathbb{R}^{n}}f(\bm x)
\]
How to find the optimal solution? The direct idea is to take the first order derivative:
\begin{align*}
\frac{\partial f}{\partial \bm x}&=\frac{1}{2}\frac{\partial(\bm x\trans\bm A\bm x)}{\partial \bm x}+\frac{\partial(\bm b\trans\bm x)}{\bm x}\\
&=\frac{1}{2}(\bm{Ax}+\bm A\trans\bm x)+\bm b.
\end{align*}
Since $\bm A$ is symmetric, we obtain
\[
\frac{\partial f}{\partial \bm x}=\bm{Ax}+\bm b.
\]
If $\bm x^{*}$ is an optimal solution, then it must satisfy:
\[
\bigtriangledown f(\bm x^{*})=\frac{\partial f(\bm x^{*})}{\partial \bm x}=\bm 0\implies
\bm A\bm x^{*}+\bm b=\bm 0.
\]
There may follow these cases:
\begin{itemize}
\item
If equation $\bm{Ax}+\bm b=\bm 0$ has no solution, then $f(\bm x)$ is unbounded.\\
This statement is remained to be proved.
\item
If equation $\bm{Ax}+\bm b=\bm 0$ has a solution $\bm x^{*}$, it doesn't mean $\bm x^{*}$ is an optimal solution. (Note that the reverse is true.)\\
Let's raise a counterexample: if we set
\[
\bm A=\begin{bmatrix}
1&0\\0&-1
\end{bmatrix},\qquad\bm b=\bm 0,\qquad\bm x=\begin{pmatrix}
x_1\\x_2
\end{pmatrix},
\]
then $f(\bm x)=\frac{1}{2}(x_1^2-x_2^2)$.\\
One solution to $\bm{Ax}+\bm b=\bm 0$ is $\bm x^*=\begin{pmatrix}
0\\0
\end{pmatrix}$. Obviously, $\bm x^{*}$ is not a optimal solution. Intutively, if we let $x_1=0,x_2\rightarrow\infty$, then $f(\bm x)\rightarrow-\infty$!
\end{itemize}
\end{example}
\subsubsection{Second optimality condition}
If $\bm x^{*}$ is a optimal solution to $f(\bm x)$, what else condition should $\bm x^{*}$ satisfy?\\ Let's take $f(\bm x)=\frac{1}{2}\bm x\trans\bm A\bm x+\bm b\trans\bm x$ as an example, we want to find $\bm x^{*}$ s.t. $\min f(\bm x)=f(\bm x^{*})$.\\
Firstly, we convert $f(\bm x)$ into its \textit{taylor expansion}:
\[
f(\bm x)=f(\bm x^{*})+\inp{\bigtriangledown f(\bm x^*)}{\bm x-\bm x^*}+\frac{1}{2}(\bm x-\bm x^*)\trans\bigtriangledown^2f(\bm x^*)(\bm x-\bm x^*).
\]
Note that $\bigtriangledown^2f(\bm x^*)$ is the Hessian matrix of $f(\bm x^*)$, which is defined as \[\bigtriangledown^2f(\bm x^*):=\begin{bmatrix}
\frac{\partial^2 f(\bm x^*)}{\partial x_{i}\partial x_{j}}
\end{bmatrix}=\bigtriangledown(\bigtriangledown f(\bm x^*)).\]
Firstly we compute $\bigtriangledown f(\bm x)$ and $\bigtriangledown^2 f(\bm x)$:
\begin{align*}
\bigtriangledown f(\bm x)&=\frac{1}{2}(\bm{Ax}+\bm A\trans\bm x)+\bm b.\\
\bigtriangledown^2 f(\bm x)&=\bigtriangledown\left[\frac{1}{2}(\bm{Ax}+\bm A\trans\bm x)+\bm b\right]=\frac{1}{2}\bigtriangledown(\bm{Ax})+\frac{1}{2}\bigtriangledown(\bm A\trans\bm x)
=\frac{1}{2}(\bm A+\bm A\trans).
\end{align*}
If we assume $\bm A$ is \emph{symmetric}, then we have $\bigtriangledown f(\bm x)=(\bm A+\bm A\trans)\bm x+\bm b$ and $\bigtriangledown^2 f(\bm x)=\bm A$.\\
Since the optimal solution $\bm x^*$ must satisfy $\bigtriangledown f(\bm x^*)=\bm 0$, we deive \[\inp{\bigtriangledown f(\bm x^*)}{\bm x-\bm x^*}=0.
\implies
f(\bm x)=f(\bm x^{*})+\frac{1}{2}(\bm x-\bm x^*)\trans\bigtriangledown^2f(\bm x^*)(\bm x-\bm x^*).
\]
Hence $f(\bm x)-f(\bm x^{*})=\frac{1}{2}(\bm x-\bm x^*)\trans\bm A(\bm x-\bm x^*).$\\
Since $\bm x^*$ is optimal that minimize $f(\bm x)$, $LHS=f(\bm x)-f(\bm x^*)\ge0$.
\[
\implies 
\frac{1}{2}(\bm x-\bm x^*)\trans\bm A(\bm x-\bm x^*)\ge0
\]
Or equivalently,
\[
(\bm x-\bm x^*)\trans\bm A(\bm x-\bm x^*)\ge0\text{ for $\forall\bm x$.}
\Longleftrightarrow
\bm x\trans\bm A\bm x\ge0 \text{ for $\forall\bm x$.}
\]
Our conclusion is that if there exists a optimal solution for $f(\bm x)$, then the matrix $\bm A$ should satisfy $\bm x\trans\bm A\bm x\ge0$ for $\forall\bm x.$ We have a specific name for such $\bm A$.
\subsection{Positive Definite Matrices}
\begin{definition}[Positive-definite]\qquad\\
\begin{itemize}
\item
Matrix $\bm A$ is \textit{positive-semidefinite} (PSD) if $\bm x\trans\bm A\bm x\ge0$ for $\forall\bm x.$ And we denote it as $\bm A\succeq0.$
\item
Matrix $\bm A$ is \textit{positive-definite} (PD) if $\bm x\trans\bm A\bm x>0$ for $\forall\bm x\ne\bm 0.$ And we denote it as $\bm A\succ0.$
\item
Matrix $\bm A$ is \textit{indefinite} if there exist some $\bm x$ and $\bm y$ s.t.
\[
\bm x\trans\bm A\bm x<0<\bm y\trans\bm A\bm y.
\]
\end{itemize}
\end{definition}
\begin{remark}
If a matrix is PSD or PD, it is usually assumed to be symmetric by default. Even in other textbooks, the definition for PSD and PD contains the \textit{symmetric} condition.
\end{remark}
\begin{theorem}\label{PD_theorem}
Let $\bm A$ be a $n\x n$ real symmetric matrix, the following are equivalent:
\begin{enumerate}
\item
$\bm A$ is PD.
\item
All eigenvalues of $\bm A$ are positive.
\item
All $n$ \textit{upper left square submatrices} $\bm A_1.\dots,\bm A_n$ all have positive determinants.
\item
$\bm A$ could be factorized as $\bm R\trans\bm R$, where $\bm R$ is nonsingular.
\end{enumerate}
\end{theorem}
You may be confused about the ``upper left submatrices'', they are the 1 by 1, 2 by 2,$\dots$,n by n submatrices of $\bm A$ on the upper left. The $n$ by $n$ submatrix is exactly $\bm A$. Before we geive a detailed proof, let's show how to test some matrices for positive definiteness by using this theorem:
\begin{example}
Test these matrices $\bm A$ and $\bm B$ for positive definiteness:
\[
\bm A=\begin{bmatrix}
1&&&\\&2&&\\&&2&\\&&&2
\end{bmatrix}\qquad\text{and}\qquad
\bm B=\begin{bmatrix}
1&-1&&\\-1&2&-1&\\&-1&2&-1\\&&-1&2
\end{bmatrix}
\]
\begin{itemize}
\item
For matrix $\bm A$, its eigenvalues are $\{1,2,2,2\}.$ So all eigenvalues of $\bm A$ are positive, $\bm A$ is PD. Moverover, we can test its positive definiteness by definition: 
\[
\bm x\trans\bm A\bm x=x_1^2+2x_2^2+2x_3^2+2x_4^2>0.
\]
for $\forall\bm x=\begin{pmatrix}
x_1&x_2&x_3&x_4
\end{pmatrix}\trans\ne\bm 0$.
\item
For matrix $\bm B$, all \textit{upper left square submatrices} is given by
\[
\bm B_1=\begin{bmatrix}
1
\end{bmatrix}\quad
\bm B_2=\begin{bmatrix}
1&-1\\-1&2
\end{bmatrix}\quad
\bm B_3=\begin{bmatrix}
1&-1&\\-1&2&-1\\&-1&2
\end{bmatrix}\quad
\bm B_4=\begin{bmatrix}
1&-1&&\\-1&2&-1&\\&-1&2&-1\\&&-1&2
\end{bmatrix}
\]
After messy computation, we obtain
\[
\det(\bm B_1)=1\quad\det(\bm B_2)=1\quad
\det(\bm B_3)=1\quad\det(\bm B_4)=1.
\]
Hence all \textit{upper left square determinants} are positive, $\bm B$ is PD.
\end{itemize}
\end{example}
\newpage
Then we begin to give a proof for this theorem:
\begin{proof}
\begin{itemize}
\item
$(1)\implies(2):$ Suppose $\lambda$ is any eigenvalue of $\bm A$. Then $\bm{Ax}=\lambda\bm x$ for some $\bm x\ne\bm 0$.\\
By postmutliplying $\bm x\trans$ both sides we obtain:
\[
\bm x\trans\bm A\bm x=\lambda\bm x\trans\bm x=\lambda\|\bm x\|^2
\implies
\lambda=\frac{\bm x\trans\bm A\bm x}{\|\bm x\|^2}>0.
\]
\item
$(2)\implies(1):$ Assume all eigenvalues $\lambda_i>0$ for $i=1,2,\dots,n.$\\
For $\forall\bm x\ne\bm 0$, our goal is to show $\bm x\trans\bm A\bm x>0:$\\
Since $\bm A$ is real symmetric matrix, we do eigendecomposition of $\bm A$:
\[
\bm A=\bm Q\Lambda\bm Q\trans\qquad\text{$\bm Q$ is orthonormal matrix.}
\]
Hence 
\[
\bm x\trans\bm A\bm x=\bm x\trans\bm Q\Lambda\bm Q\trans\bm x=(\bm Q\trans\bm x)\trans\Lambda(\bm Q\trans\bm x).
\]
If we set $\tilde{\bm x}=\bm Q\trans\bm x=\begin{bmatrix}
\tilde{x_1}&\dots&\tilde{x_n}
\end{bmatrix},$ then $\bm x\trans\bm A\bm x$ can be written as
\[
\bm x\trans\bm A\bm x=\tilde{\bm x}\trans\Lambda\tilde{\bm x}=\sum_{i=1}^{n}\lambda_i\tilde{x_i}^2\ge0.
\]
Then we aruge that $\sum_{i=1}^{n}\lambda_i\tilde{x_i}^2\ne0.$ Actually we only need to show $\|\bm x\|\ne0:$\\
Since previously we have shown $\|\bm Q\trans\bm x\|=\|\bm x\|$, we obtain:
\[
\|\tilde{\bm x}\|=\|\bm Q\trans\bm x\|=\|\bm x\|\ne0.
\]
\item
$(1)\implies(3):$ We only need to show $\det(\bm A_k)>0$ for $k=1,\dots,n$.\\
Given any $\tilde{\bm x}=\begin{pmatrix}
x_1\\x_2\\\vdots\\x_k
\end{pmatrix}\in\mathbb{R}^{k}$, we construct $\bm x=\begin{pmatrix}
\tilde{\bm x}\\\bm 0
\end{pmatrix}\in\mathbb{R}^{n}$.\\
Since $\bm A\succ0$, we find
\begin{align*}
\bm x\trans\bm A\bm x&=\begin{pmatrix}
\tilde{\bm x}\trans&\bm 0
\end{pmatrix}\bm A\begin{pmatrix}
\tilde{\bm x}\\\bm 0
\end{pmatrix}\\
&=\tilde{\bm x}\trans\bm A_k\tilde{\bm x}>0.
\end{align*}
Since $\tilde{\bm x}$ is arbitrary vector in $\mathbb{R}^{k}$, we derive $\bm A_k\succ0.$\\
By $(2)$ of this theorem, all eigenvalues of $\bm A_k$ are positive. \\ Thus $\det(\bm A_k)=\text{product of all eigenvalues of $\bm A$}>0.$
\item
$(3)\implies(4):$
\begin{itemize}
\item
We want to show all pivots of $\bm A$ are positive first:\\
We do row transform to convert $\bm A$ into upper triangular matrix $\tilde{\bm A}$:
\[
\begin{bmatrix}
\x&\x&\x\\
\x&\x&\x\\
\x&\x&\x
\end{bmatrix}\Longrightarrow
\begin{bmatrix}
\x&\x&\x\\
0&\x&\x\\
0&0&\x
\end{bmatrix}
\]
And during the row transformation, the determinant doesn't change. Moreover, the correponding \textit{upper left submatrices} determinants don't change. In other words, we obtain
\[
\det(\tilde{\bm A}_i)=\det(\bm A_i)\text{ for }i=1,\dots,n.
\]
And moreover, $\tilde{\bm A_i}$ always contains $\tilde{\bm A}_{i-1}$ on its upper left side:
\[
\tilde{\bm A}_i=\begin{bmatrix}
\tilde{\bm A}_{i-1}&\bm B\\\bm0&\tilde{a}_{ii}
\end{bmatrix}
\]
And we notice $\tilde{\bm A}_i$'s are also upper triangular matrix. The determinant of a upper triangular matrix is the product of its diagonal entries. Hence we obtain
\[
\det(\tilde{\bm A}_i)=\tilde{a_{ii}}\det(\tilde{\bm A}_{i-1})\text{ for }i=2,\dots,n.
\]
Thus $\tilde{a_{ii}}=\frac{\det(\tilde{\bm A}_i)}{\det(\tilde{\bm A}_{i-1})}=\frac{\det(\bm A_i)}{\det(\bm A_{i-1})}$ for $i=2,\dots,n.$\\ Due to $(3)$ of this theorem, $\tilde{a_{ii}}>0$ for $i=2,\dots,n.$ And $\tilde{a_{11}}=\det(\tilde{\bm A}_1)=\det(\bm A_1)>0$.\\
In conclusion, all pivots $\tilde{a_{ii}}>0$ for $i=1,\dots,n.$
\item
Then we do the LDU composition for $\bm A$. Since $\bm A$ is symmetric, we obtain
\[
\bm A=\bm L\bm D\bm L\trans
\]
where $\bm D=\diag(d_1,\dots,d_n)$. The diagonal entries of $\bm D$ are pivots of $\bm A$. And $\bm L$ is lower triangular matrix with 1's on the diagonal entries.\\
Since all pivots of $\bm A$ are positive, we define $\sqrt{\bm D}:=\diag(\sqrt{d_1},\dots,\sqrt{d_n})$.\\
Hence $\bm A$ could be written as:
\[
\bm A=\bm L\begin{pmatrix}
d_1&&\\&\ddots&\\&&d_n
\end{pmatrix}\bm L\trans=\bm L\sqrt{\bm D}\sqrt{\bm D}\bm L\trans=(\sqrt{\bm D}\bm L\trans)\trans(\sqrt{\bm D}\bm L\trans).
\]
Fefine $\bm R=\sqrt{\bm D}\bm L\trans$. Since $\sqrt{\bm D}$ and $\bm L\trans$ are nonsingular, $\bm D$ is nonsingular. \\Hence $\bm A=\bm R\trans\bm R$, where $\bm R$ is nonsingular matrix.
\end{itemize}
\item
$(4)\implies(1): $
Suppose $\bm A=\bm R\trans\bm R$, where $\bm R$ is nonsingular. Then for any $\bm x\in\mathbb{R}^{n}$, we have
\[
\bm x\trans\bm A\bm x=\bm x\trans\bm R\trans\bm R\bm x=\|\bm{Rx}\|^2\ge0.
\]
Then we only need to show that if $\bm x\ne\bm0$, then $\|\bm{Rx}\|\ne0.$:\\
Since $\bm R$ is nonsinguar, when $\bm x\ne\bm 0$, we obtain $\bm{Rx}\ne\bm0.$ Hence $\|\bm{Rx}\|\ne0.$
\end{itemize}
\end{proof}
We may ask is there any quick ways to determine the positive definiteness of a matrix? The answer is yes. Let's introduce some definitions first:
\enlargethispage{1cm}
\begin{definition}[Submatrix]\qquad\\
If $\bm A$ is a $n\x n$ matrix, then a submatrix of $\bm A$ is obtained by keeping some collection of rows and columns.
\end{definition}
\begin{example}
If $\bm A=\begin{bmatrix}
1&-1&&\\-1&2&-1&\\&-1&2&-1\\&&-1&2
\end{bmatrix}$, then if we keep the (1,3,4)th row of $\bm A$ and (1,2)th column of $\bm A$, our submatrix is denoted as
\[
\bm A_{(1,3,4),(1,2)}=\begin{bmatrix}
1&-1\\0&-1\\0&0
\end{bmatrix}
\]
\end{example}
\begin{definition}[principal submatrix]
If $\bm A$ is a $n\x n$ matrix, then a principal submatrix of $\bm A$ is obtained by keeping the same collection of rows and columns. For example, if we want to keep the (5,7)th row of $\bm A$, in order to construct a principal submatrix, we must keep the (5,7)th column of $\bm A$ as well.
\end{definition}
\begin{example}
If $\bm A=\begin{bmatrix}
1&-1&&\\-1&2&-1&\\&-1&2&-1\\&&-1&2
\end{bmatrix}$, then if we keep the (1,3,4)th row of $\bm A$, in order to construct a principal submatrix, we have to keep (1,3,4)th column of $\bm A$ as well. Our principal submatrix is denoted as
\[
\bm A_{(1,3,4),(1,3,4)}=\begin{bmatrix}
1&0&0\\0&2&-1\\0&-1&2
\end{bmatrix}
\]
\end{example}
\begin{definition}[leading principal submatrix]
If $\bm A$ is a $n\x n$ matrix, then a leading principal submatrix of $\bm A$ is obtained by keeping the first $k$ rows and columns of $\bm A$, where $k\in\{1,2,\dots,n\}.$
\end{definition}
Note that the leading principal submatrix is just the upper left submatrix we have mentioned before.
\begin{corollary}
If $\bm A\succ0$, then all principal submatrices of $\bm A\succ0.$
\end{corollary}
\begin{proof}
Our goal is to show $\bm A_{\alpha,\alpha}\succ0$, where $\alpha\in\{1,2,\dots,n\}$.\\
For any $\bm x_{\alpha}\in\mathbb{R}^{|\alpha|}$, we only need to show $\bm x_{\alpha}\trans\bm A_{\alpha,\alpha}\bm x_{\alpha}>0.$ Note that $|\alpha|$ denotes the number of elements in set $\alpha$.\\
We construct $\bm x\in\mathbb{R}^n$ s.t. the $i$th entry of $\bm x$ is
\[
\bm x_i=\left\{
\begin{aligned}
(\bm x_{\alpha})_i&\qquad i\in\alpha\\
0&\qquad i\notin\alpha
\end{aligned}\right.
\]
It's obvious that
\begin{align*}
\bm x\trans\bm A\bm x&=
\sum_{i,j=1}^{n}\bm x_i\bm x_j\bm A_{ij}\\
&=\sum_{i,j\in\alpha}(\bm x_{\alpha})_i(\bm x_{\alpha})_j(\bm A_{\alpha,\alpha})_{ij}\\
&=\bm x_{\alpha}\trans\bm A_{\alpha,\alpha}\bm x_{\alpha}>0.
\end{align*}
\end{proof}
How to use this corollary to test the positive definiteness?\\
For example, given $\bm A=\begin{bmatrix}
2&-1&1\\-1&0&0\\1&0&1
\end{bmatrix}$, immediately we find one principal matrix is $\bm A_{2,2}=0.$ Hence it is not PD.\\
Also, there are many equivalent statements related to PSD.
\begin{theorem}
Let $\bm A$ be a $n\x n$ real symmetric matrix, the following are equivalent:
\begin{enumerate}
\item
$\bm A$ is PSD.
\item
All eigenvalues of $\bm A$ are nonnegative.
\item
$\bm A$ could be factorized as $\bm R\trans\bm R$, where $\bm R$ is square.
\end{enumerate}
\end{theorem}
\begin{remark}
Is $\bm A\succeq0$ equivalent to $\bm A_{ij}\ge0$? No. Let's raise a counterexample:
\[
\bm A=\begin{bmatrix}
1&-0.5\\-0.5&1
\end{bmatrix}\succeq0.
\]
\end{remark}
PSD has many interesting properties. Before we introduce one, let's extend the definiton of inner product into matrix form:
\begin{definition}[Frobenius inner product]
For two real $n\x n$ matrix $\bm A$ and $\bm B$, the \emph{Frobenius inner product} is given by
\[
\inp{\bm A}{\bm B}=\sum_{i,j=1}^{n}\bm A_{ij}\bm B_{ij}
\]
\end{definition}
\begin{proposition}
If two real $n\x n$ symmetric matrix $\bm A\succeq0,\bm B\succeq0$, then $\inp{\bm A}{\bm B}\ge0.$
\end{proposition}
\begin{proof}
Since $\bm A\succeq0$, there exists square matrix $\bm R=\begin{bmatrix}
\bm r_1&\dots&\bm r_n
\end{bmatrix}$ s.t.
\[
\bm A=\bm R\bm R\trans=\sum_{k=1}^{n}\bm r_k\bm r_k\trans
\]
Hence our inner product is given by
\begin{align*}
\inp{\bm A}{\bm B}&=\inp{\sum_{k=1}^{n}\bm r_k\bm r_k\trans}{\bm B}\\
&=\sum_{k=1}^{n}\inp{\bm r_k\bm r_k\trans}{\bm B}\\
&=\sum_{k=1}^{n}(\sum_{i,j=1}^{n}\bm B_{ij}\bm R_{ki}\bm R_{kj})\\
&=\sum_{k=1}^{n}\bm r_k\trans\bm B\bm r_k
\end{align*}
Since $\bm B\succeq0$, we obtain $\inp{\bm A}{\bm B}=\sum_{k=1}^{n}\bm r_k\trans\bm B\bm r_k\ge0.$
\end{proof}