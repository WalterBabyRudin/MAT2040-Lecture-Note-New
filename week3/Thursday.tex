
%\chapter{Week3}

\section{Thursday}\index{week3_Thursday_lecture}
\subsection{Review}
The last lecture you may be confused about how to compute the null space $N(\bm A)$, i.e., why we follow the proceed to compute special solutions $y_i$. Let's review the whole steps for solving rectangular by using block matrix form.

\begin{itemize}
\item
After converting the matrix $\bm A$ into the rref form $\bm R$, without loss of generality, we could convert the rref into the form $\begin{bmatrix}
\bm I&\bm B\\\bm 0&\bm 0
\end{bmatrix}$ by switching columns.
\begin{example}
Last time our rref is given by:
\[
\bm R = \begin{bmatrix}
1&1&0&1\\0&0&1&1\\0&0&0&0
\end{bmatrix}
\]
We notice that column 3 is pivot column, so we can switch it into the second column. (By switching column 2 and column 3):
\[
\bm R\implies \begin{bmatrix}
1&0&3&-1\\0&1&0&1\\0&0&0&0
\end{bmatrix} = \begin{bmatrix}
\bm I&\bm B\\\bm 0&\bm 0
\end{bmatrix}
\]
\end{example}
\item
Thus our system could be written into the form:
\begin{equation}
\bm{Rx} = \bm c\implies \begin{bmatrix}
\bm I&\bm B\\\bm 0&\bm 0
\end{bmatrix}\begin{bmatrix}
x_1\\x_2\\x_3\\x_4
\end{bmatrix} = \begin{bmatrix}
c_1\\c_2\\c_3
\end{bmatrix}\label{Eq:3:1}
\end{equation}

Since we have changed the columns of $\bm R$, so the row $2$ and row $3$ of $\bm x$ is also switched respectively. Thus $x_1$ and $x_2$ are pivot variables,  and $x_3$ and $x_4$ are free variables of $\bm x$. From (\ref{Eq:3:1}) we derive:
\[
\left\{\begin{aligned}
\bm I\begin{bmatrix}
x_1\\x_2
\end{bmatrix}+ \bm B\begin{bmatrix}
x_3\\x_4
\end{bmatrix} &= \begin{bmatrix}
c_1\\c_2
\end{bmatrix}\\
0 &= c_3
\end{aligned}\right.
\]

\item
If $c_3\ne 0$, then there is no solution; so let's preassume $c_3=0$. Then \textit{pivot variables} could be expressed as the form of \textit{free variables}:
\[
\begin{pmatrix}
x_1\\x_2
\end{pmatrix} = \begin{pmatrix}
c_1\\c_2
\end{pmatrix} - \bm B\begin{pmatrix}
x_3\\x_4
\end{pmatrix}
\]
Suppose $-\bm B = \begin{bmatrix}
\bm{\hat y_1}&\bm{\hat y_2}
\end{bmatrix}$, then pivot variables can be expressed as:
\[
\begin{pmatrix}
x_1\\x_2
\end{pmatrix} = \begin{pmatrix}
c_1\\c_2
\end{pmatrix}+x_3\bm{\hat y_1} + x_4\bm{\hat y_2}
\]
\item
Therefore, the complete solution to the system is given by
\begin{align}
\bm x=\begin{pmatrix}
x_1\\x_2\\0\\0
\end{pmatrix}
+
\begin{pmatrix}
0\\0\\x_3\\x_4
\end{pmatrix}
 &= \begin{pmatrix}
c_1\\c_2\\0\\0
\end{pmatrix} +  \begin{pmatrix}
x_3\bm{\hat y_1} + x_4\bm{\hat y_2}\\0\\0
\end{pmatrix} + \begin{pmatrix}
0\\0\\x_3\\x_4
\end{pmatrix}\\
&=\begin{pmatrix}
c_1\\c_2\\0\\0
\end{pmatrix} +  x_3\begin{pmatrix}
\bm{\hat y_1}\\0\\0
\end{pmatrix}+x_4\begin{pmatrix}
\bm{\hat y_2}\\0\\0
\end{pmatrix} + x_3\begin{pmatrix}
0\\0\\1\\0
\end{pmatrix} + x_4\begin{pmatrix}
0\\0\\0\\1
\end{pmatrix}\\
&=\underbrace{\begin{pmatrix}
c_1\\c_2\\0\\0
\end{pmatrix}}_{\bm x_p}
+\underbrace{x_3\begin{pmatrix}
\bm{\hat y_1}\\1\\0
\end{pmatrix} + x_4\begin{pmatrix}
\bm{\hat y_2}\\0\\1
\end{pmatrix}}_{\bm x_{\text{special}}}
\end{align}                                          
where $x_3$ and $x_4$ could be arbitarary.
\item
We can verify our computed special solutions is true by matrix multiplication:
\[
\begin{aligned}
\mbox{Special Solution Matrix: }&
\begin{pmatrix}
\bm{\hat y_1}&\bm{\hat y_2}\\1&0\\0&1
\end{pmatrix}
= \begin{pmatrix}
-\bm B\\\bm I
\end{pmatrix}
\\
\mbox{Verification: }&
\begin{pmatrix}
\bm I&\bm B\\\bm 0&\bm 0
\end{pmatrix}\begin{bmatrix}
-\bm B\\\bm I
\end{bmatrix} = \begin{bmatrix}
-\bm B+\bm B\\\bm 0
\end{bmatrix} = \begin{bmatrix}
\bm 0\\\bm 0
\end{bmatrix}
\end{aligned}
\]
\end{itemize}
Open Question: 
If our rectangular matrix is $m\x n(m>n)$, how to solve it?

Answer: Similarly, we do G.E. to get rref. After switching columns, it will be of the form:
\[
\begin{bmatrix}
1&&\\&\ddots&\\&&1\\0&\dots&0
\end{bmatrix}\text{ or }\begin{bmatrix}
1& & & & \\
 &\ddots&&&\\
 & & 1&&\\
 &&&0&\\
 &&&&0\\0&0&0&\dots&0
\end{bmatrix}
\]
\subsection{Remarks on solving linear system equations}
\emph{
There are two kinds of linear equations, and the classification criteria depends on $m$ and $n$:
}
\begin{theorem}
Let $m$ denotes the number of equations, $n$ denotes the number of variables. For the number of solutions for $\bm{Ax} = \bm b$, where $\bm A\in\mathbb{R}^{m\x n}$, we obtain:
\begin{itemize}
\item
$m<n$: either \emph{no solution} or \emph{infinitely many solutions}
\item
$m\ge n$: \emph{no solution}; \emph{unique solution} ($N(\bm A) =\bm 0$); or \emph{infinitely many solutions}.
\end{itemize}
\end{theorem}
We prove for the $m<n$ case first:
\begin{proof}[Proofoutline for $m<n$ case:]
Recall that we can convert $\bm{Ax} = \bm b$ into $\bm{Rx} = \bm{c}$. WLOG, we switch columns of $\bm R$ to put pivot columns in the left-most:
\[
\begin{bmatrix}
1&&&\x&\x\\&\ddots&&\x&\x\\&&1&\x&\x\\0&0&0&0&0\\\dots&&&&\\0&0&0&0&0
\end{bmatrix}\bm x = \begin{bmatrix}
c_1\\\vdots\\c_r\\c_{r+1}\\\vdots\\c_n
\end{bmatrix},
\]
where $x_1.x_2.\dots,x_r$ are pivot variables. Hence, we have $(n-r)$ free variables, and $N(\bm A)$ is spanned by $(n-r)$ special vectors $y_1,y_2,\dots,y_{n-r}$.

It suffices to show that the $m<n$ rectangular system does not have unique solution, i.e., $N(\bm A)>0$. It suffices to show $n>r$.

Obviously, $r\le m$, and we have $n>m$, so we obtain $n>r$.
\end{proof}
Equivalently, we obtain the proposition and the corollary below:
\begin{proposition}
For system $\bm{Ax} = \bm b$, where $\bm A\in\mathbb{R}^{m\x n},$ $m<n$,\\
it either has no solution or infinitely many solutions.
\end{proposition}
\begin{corollary}\label{corollary_infinite_condition}
For system $\bm{Ax} = \bm 0$, where $\bm A\in\mathbb{R}^{m\x n},$ $m<n$,\\
\hspace*{1cm} it always has infinitely many solutions.
\end{corollary}

\subsubsection{What is r?}
We ask the question again, what is $r$? Let's see some examples before answering this question.
\begin{example}
If we want to solve system of equations of size $1000$ as the following:
\[
\left\{\begin{aligned}
x_1+x_2&=3\\2x_1+2x_2&=6\\\dots\\1000x_1+1000x_2&=3000
\end{aligned}\right.
\]
It seems very difficult when hearding it has 1000 equations, but the remaining 999 equations could be redundant (They actually don't exist):
\[
\begin{bmatrix}
1&1\\2&2\\\vdots&\vdots\\1000&1000
\end{bmatrix}\implies
\begin{bmatrix}
1&1\\0&0\\\vdots&\vdots\\0&0
\end{bmatrix}
\]
\end{example}
Here we see that only one equation $x_1+x_2=3$ is real, the remaining part is not real. So we claim that $r$ is the number of ``real'' equations. But what is the definition for ``real'' equations? Let's discuss the definition for \textit{linear dependence} first.
\subsection{Linear dependence}
\begin{definition}[linear dependence]
The vectors $\bm v_1,\bm v_2,\dots,\bm v_n$ in linear space $\bm V$ are \emph{linearly dependent} if there exists $c_1,c_2,\dots,c_n\in\mathbb{R}$ s.t.
\[
c_1\bm v_1+c_2\bm v_2 +\dots+c_n\bm v_n = \bm 0.
\]
In other words, it means one of $v_i$ could be expressed as the linear combination of others. Assume $c_n\ne 0$, we can express $\bm v_n$ as:
\[
\bm v_n = -\frac{c_1}{c_n}\bm v_1-\frac{c_2}{c_n}\bm v_2 -\dots-\frac{c_{n-1}}{c_n}\bm v_{n-1}.
\]
\end{definition}
\begin{definition}[linear independence]
The vectors $\bm v_1,\bm v_2,\dots,\bm v_n$ in linear space $\bm V$ are \emph{linearly independent} if the equation
\[
c_1\bm v_1+c_2\bm v_2 +\dots+c_n\bm v_n = \bm 0
\]
only has the trivial solution $c_1=c_2=\dots=c_n=0$.

In other words, if $\bm v_1,\bm v_2,\dots,\bm v_n$ are not \emph{linearly dependent}, they must be \emph{linearly independent}.
\end{definition}
\begin{remark}
Note that \emph{only} in this course, if we say vectors are dependent, we mean they are \emph{linearly} dependent. In other courses dependent may have other definitions. In the following lectures, we simplify the noun 
\textit{dependent} as \textit{dep.}; and the noun \textit{independent} as \textit{ind.}
\end{remark}
Here we pick some examples to help you understand dep. and ind.:
\begin{example}
\begin{itemize}
\item
$\bm v_1=(1,1)$ and $\bm v_2 = (2,2)$ are \emph{dep.} because
\[
(-2)\x\bm v_1 + \bm v_2 = \bm 0.
\]
\item
The only one vector $\bm v_1=2$ is \emph{ind.} because 
\[
c\bm v_1 = \bm 0
\Longleftrightarrow
c=0.
\]
\item
The only one vector $\bm v_1=0$ is \emph{dep.} because
\[
2\x\bm v_1 = \bm 0
\]
\item
$\bm v_1 = (1,2)$ and $\bm v_2 = (0,0)$ are \emph{dep.} because
\[
0\x\bm v_1 + 1\x\bm v_2 = \bm 0.
\]
\item
The upper triangular matrix $\bm A = \begin{bmatrix}
3&4&2\\0&1&5\\0&0&2
\end{bmatrix}$ has three column vectors:
\[
\bm v_1 = \begin{bmatrix}
3\\0\\0
\end{bmatrix},\bm v_2 = \begin{bmatrix}
4\\1\\0
\end{bmatrix},\bm v_3 = \begin{bmatrix}
2\\5\\2
\end{bmatrix}
\]
$\bm v_1,\bm v_2,\bm v_3$ are \emph{ind.} because
\[
c_1\bm v_1 + c_2\bm v_2 + c_3\bm v_3 = \bm 0
\Longleftrightarrow
c_1=c_2=c_3=0. (\text{Why? because $\bm A$ is invertible})
\]
\end{itemize}
\end{example}
\subsubsection{Remarks}
\paragraph{How many solutions meet the linear dependence criteria?}
Recall that in last week we have studied that the following statements are equivalent: ()
\begin{itemize}
\item
Vectors $a_1,a_2,\dots,a_n\in\mathbb{R}^{m}$ are dep.
\item
$\exists$ nonzero $\bm c$ s.t. $\sum_{i=1}^{n}c_ia_i = \bm 0$.
\item
$\exists$ $\bm c\ne\bm 0$ s.t.
\[
\bm{Ac} := \left[\begin{array}{c|c|c}
a_1&\dots&a_n
\end{array}\right]\bm c = \bm 0
\]
\end{itemize}
For the third statement, if we could choose one $\bm c$, then how many $\bm c$ can we choose? For the $m<n$ case, by corollary (\ref{corollary_infinite_condition}), we obtain:
\begin{corollary}
When vectors $a_1,a_2,\dots,a_n\in\mathbb{R}^{m}(m<n)$ are dependent, there exists infinitely solutions $c_1,c_2,\dots,c_n$ such that $\sum_{i=1}^{n}c_ia_i = \bm 0$.
\end{corollary}
\paragraph{The real equations are essentially those linearly independent equations}
\subsection{Basis and dimension}
\begin{definition}[Basis]
The vectors $v_1,\dots,v_n$ form a \emph{basis} for a vector space $\bm V$ if and only if:
\begin{enumerate}
\item
$v_1,\dots,v_n$ are \emph{linearly independent}.
\item[]
and
\item
$v_1,\dots,v_n$ span $\bm V$.
\end{enumerate}
\end{definition}
\begin{example}
In $\mathbb{R}^{3}$, 
\begin{itemize}
\item
$\begin{bmatrix}
1\\0\\0
\end{bmatrix},\begin{bmatrix}
0\\1\\0
\end{bmatrix},\begin{bmatrix}
0\\0\\1
\end{bmatrix}$ form a basis.
\item
$\begin{bmatrix}
1\\0\\0
\end{bmatrix}$ is not a basis, since it doesn't span $\mathbb{R}^{3}$.
\item
$\begin{bmatrix}
1\\0\\0
\end{bmatrix},\begin{bmatrix}
0\\1\\0
\end{bmatrix},\begin{bmatrix}
0\\0\\1
\end{bmatrix},\begin{bmatrix}
2\\0\\0
\end{bmatrix}$ don't form a basis, since they aren't linearly independent.
\item
$\begin{bmatrix}
1\\0\\0
\end{bmatrix},\begin{bmatrix}
1\\2\\3
\end{bmatrix},\begin{bmatrix}
2\\1\\3
\end{bmatrix}$ form a basis.
\end{itemize}
\end{example}
We find that the number of vectors for the basis of $\mathbb{R}^{3}$ is always 3, is this a coincidence? The theorem below gives the answer.
\begin{theorem}\label{theorem_3.2}
If $v_1,v_2,\dots,v_m$ is a basis; and $w_1,w_2,\dots,w_n$ is another basis for the same vector space $\bm V$, then $n=m$.
\end{theorem}
In order to proof it, let's try simple case first:
\begin{proof}[proofoutline.]
\begin{enumerate}
\item
In order to proof it, let's try simple case first:
\begin{itemize}
\item
Consider $\bm V = \mathbb{R}$ case first:
For $\mathbb{R}$, the number 1 forms a basis. Let's show that 2 vectors in $\mathbb{R}$ cannot be a basis:
\begin{itemize}
\item
Given any two vectors $x$ and $y$, they are not a basis for $\mathbb{R}$, since that
\begin{itemize}
\item
if $x=0$ or $y=0$, they are not ind.
\item
otherwise, $y = \frac{y}{x}\x x\implies \frac{y}{x}\x x + (-1)\x y = 0$. So they are not ind.
\end{itemize}
\end{itemize}
\item
Then we consider $\bm V = \mathbb{R}^3$ case:

For $\mathbb{R}^3$, $\begin{bmatrix}
1\\0\\0
\end{bmatrix},\begin{bmatrix}
0\\1\\0
\end{bmatrix},\begin{bmatrix}
0\\0\\1
\end{bmatrix}$ is a basis. Our goal is to show that if $v_1,v_2,\dots,v_m$ is a basis, then $m=3$.
\begin{itemize}
\item
Let's show that $m=4$ is impossible, i.e., $4$ vectors in $\mathbb{R}^{3}$ cannot be a basis.):
\begin{itemize}
\item
It suffices to show that for $\forall a_1,a_2,a_3,a_4\in\mathbb{R}^{3}$ they must be dep.
\item
Or equivalently, $\bm{Ax}=\bm 0$ has nonzero solutions, where $\bm A = \left[\begin{array}{c|c|c|c}
a_1&a_2&\dots&a_4
\end{array}\right]\in \mathbb{R}^{3\x 4}$, which is true by corollary (\ref{corollary_infinite_condition}).
\end{itemize}
\item
Similarly, we could show any basis for $\mathbb{R}^{3}$ satisfies $m\le 3$ (i.e., m=4,5,$\dots$ is impossible).
\item
Then let's show that $m=2$ is impossible, i.e., 2 vectors in $\mathbb{R}^{2}$ cannot be a basis:
\begin{itemize}
\item
It suffics to show that for $\forall a_1,a_2\in \mathbb{R}^{3}$, they cannot span the whole space.
\item
Otherwise, $\bm{Ax} = \bm b$ must have solution for arbitrary $\bm b\in\mathbb{R}^3$, where $\bm A = \left[\begin{array}{c|c}
a_1&a_2
\end{array}\right]\in \mathbb{R}^{3\x 2}$.
\item
However, this kind system may have no solution, which is a contradiction.
\end{itemize}
\item
Similarly, we could show any basis for $\mathbb{R}^{3}$ satisfies $m\ge 3$.
\end{itemize}
\item
The same arugment could show any basis for $\mathbb{R}^{n}$ satisfies $m=n$.
\end{itemize}
\item
Next, let's consider general vector space. We assume that $n<m$ (by contradiction method).

Given that $v_1,\dots,v_n$ and $w_1,\dots,w_m$ are the basis of $\bm V$, our goal is to construct a contradiction that $w_1,\dots,w_m$ cannot form a basis.

It suffices to show that $\exists (\mbox{construct})$ $\bm c = \begin{bmatrix}
c_1&c_2&\dots&c_m
\end{bmatrix}\trans\ne\bm 0$ s.t.
\begin{equation}\label{combination}
c_1w_1+c_2w_2+\dots+c_mw_m = 0.
\end{equation}
Moreover, we can express $w_1,\dots,w_m$ in form of $v_1,\dots,v_n$:
\begin{equation}\label{basis_for_w}
\left\{\begin{aligned}
w_1 &= a_{11}v_1 +\dots+a_{1n}v_n\\
\dots\\
w_m &= a_{m1}v_1 +\dots + a_{mn}v_n
\end{aligned}\right.
\end{equation}
By (\ref{basis_for_w}), we can write (\ref{combination}) as:
\[
\begin{aligned}
0 &= \sum_{j=1}^{m}c_jw_j\\
  &= \sum_{j=1}^{m}c_j(\sum_{i=1}^{n}a_{ji}v_i)\\
  &= \sum_{j=1}^{m}\sum_{i=1}^{n}c_ja_{ji}v_i\\
  &= \sum_{i=1}^{n}\sum_{j=1}^{m}c_ja_{ji}v_i\\
  &= \sum_{i=1}^{n}v_i\x(\sum_{j=1}^{m}c_ja_{ji})\\
  &= v_1\x(\sum_{j=1}^{m}c_ja_{j1}) + v_2\x(\sum_{j=1}^{m}c_ja_{j2}) +\dots +v_n\x(\sum_{j=1}^{m}c_ja_{jn})
\end{aligned}
\]
So, in order to let LHS=0, we only need to let each of RHS=0, i.e., 
\begin{equation}
\sum_{j=1}^{m}c_ja_{j1} = \sum_{j=1}^{m}c_ja_{j2} = \dots = \sum_{j=1}^{m}c_ja_{jn} = 0.\label{Eq:3:7}
\end{equation}
In order to construct $c_j$, we write (\ref{Eq:3:7}) into matrix form:
\[
\begin{array}{lll}
\bm A\trans \bm c = \bm 0,
\mbox{ where}
&
\bm A = \begin{bmatrix}
a_{ij}
\end{bmatrix}_{1\le i\le m;1\le j\le n},
&
\bm c = \begin{bmatrix}
c_1&c_2&\dots&c_m
\end{bmatrix}\trans.
\end{array}
\]
The system $\bm A\trans \bm c = \bm 0$ has infinitie nonzero solutions by corollary (\ref{corollary_infinite_condition}). Hence we could construct infinitely such $c_j$.
\end{enumerate}
\end{proof}
During the proof, we face two difficulties:
\begin{enumerate}
\item
For arbitararily $\bm V$, we write a concrete form to express $w_1,w_2,\dots,w_m$.
\item
We write into matrix form to express $\sum_{j=1}^{m}c_ja_{j1} = \sum_{j=1}^{m}c_ja_{j2} = \dots = \sum_{j=1}^{m}c_ja_{jn} = 0.$
\end{enumerate}
Since any basis for $\bm V$ contains the same number of vectors, we can define the number of vectors to be dimension:
\begin{definition}[Dimension]
The \emph{dimension} for a vector space is the number of vectors in a basis for it.
\end{definition}
\begin{remark}
Remember that vector space $\{0\}$ has dimension $0$.\\In order to denote the dimension for a given vector space $\bm V$, we often write it as dim($\bm V$).
\end{remark}
\begin{example}
\begin{itemize}
\item
$\mathbb{R}^{n}$ has dimension $n$.
\item
$\{\text{All $m\x n$ matrix}\}$ has dimension $m\cdot n$.
\item
$\{\text{All $n\x n$ symmetric matrix}\}$ has dimension $\frac{n(n+1)}{2}$.
\item
Let $\bm P$ denote the vector space of all polynomials $f(x) = a_0+a_1x+\dots+a_nx^n$.\\
dim($\bm P$)$\ne 3$ since $1,x,x^2,x^3$ are ind.\\
The same argument can show dim($\bm P$) doesn't equal to any real number, so dim($\bm P$)=$\infty$
\end{itemize}
\end{example}
Human beings often ask a question: for a line and a plane, which is bigger?
\paragraph{Does plane has more point than a line?}
No, Cantor syas they have the same ``number'' of points by constructing one-to-one mapping.

Furthermore, $\mathbb{R},\mathbb{R}^2,\dots,\mathbb{R}^n$ has the same number of points.
\paragraph{Plane and line have different dimensions}
However, a plane has more dimensions than a line. So from this point of view, a plane is bigger than a line.


You should know some common knowledge for dimension:
\begin{enumerate}
\item
Programmer lives in $\bm 2$ dimension world. (They only live with binary.)
\item
Engineer lives in $\bm 3$ dimension world. (They only live with enign.)
\item
Physician lives in $\bm 4$ dimension world. (They discuss time.)
\item
String theories states that our world is $\bm{11}$ or $\bm{26}$ dimension, which has been proved by Qingshi Zhu.
\end{enumerate}
\paragraph{What is rank?}
Finally let's answer the question: What is rank?
\begin{quotation}
\emph{rank $=$ dimension of row space of a matrix.}
\end{quotation}
We will discuss it in the next lecture.





















