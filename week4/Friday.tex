
\chapter{Week4}

\section{Friday}\index{week4_Friday_lecture}
\subsection{Linear Transformation}
We start with a matrix $\bm A$. When multiplying $\bm A$ with a vector $\bm v$, it essentially transforms $\bm v$ to another vector $\bm{Av}$. Matrix multiplication $L(\bm v) = \bm{Av}$ gives a \emph{linear transformation}:
\begin{definition}[linear transformation]
A transformation $L$ assigns an output $T(\bm v)$ to each inpout vector $\bm v$ in $\bm V$.\\
The transformation $L(\cdot)$ is siad to be a \emph{linear transformation} if it satisfies
\[
L(\alpha \bm v_1 + \beta \bm v_2)=\alpha L(\bm v_1)+\beta L(\bm v_2)
\]
for all vectors $v_1,v_2$ and scalars $\alpha,\beta$.
\end{definition}
\emph{Key Observation:} If the input is $\bm v = \bm 0$, the output must be $L(\bm v) = \bm 0$.
\subsubsection{The idea of linear transformation}
Given the linear transformation $L: \mathbb{R}^{n}\mapsto\mathbb{R}^{m}$, let's show that in order to study the output, it suffices to start from the \emph{basis} of our output: 

Assume the basis of $\mathbb{R}^{n}$ is $\{e_1,e_2,\dots,e_n\}$, where $L(e_i)=a_i\in\mathbb{R}^{m}$ for $i=1,\dots,n$. \emph{The linearity of transformation extends to the combinations of $\bm n$ vectors.}

Hence given any vector $\bm x=x_1e_1+x_2e_2+\dots+x_ne_n\in\mathbb{R}^{n}$, we can express its transformation in matrix multiplication form:
\[
\begin{aligned}
L(\bm x) &= L(x_1e_1+x_2e_2+\dots+x_ne_n)\\
			 &= x_1L(e_1)+x_2L(e_2)+\dots+x_nL(e_n)\\
			 &= x_1a_1+x_2a_2+\dots+x_na_n=\begin{bmatrix}
a_1&a_2&\dots&a_n
\end{bmatrix}\begin{bmatrix}
x_1\\x_2\\\dots\\x_n
\end{bmatrix}\\
			 &= \bm{Ax}
\end{aligned}
\]
where $a_i:=L(e_i)$, and $\bm A$ is a $m\x n$ matrix with columns $a_1,\dots,a_n$.

\subsubsection{Matrix defines linear transformation}
Conversely, given $m\x n$ matrix $\bm A$, $L(\bm x) = \bm{Ax}$ defines a linear mapping. This is because matrix multiplication is also a linear operator.

\begin{remark}
Transformations have a new ``language''. For example, for \textit{nonlinear} transformation, if there is \emph{no matrix}, we cannot talk about \emph{column space}. But this idea could be rescued. We know the \textit{column space} consists of all outputs $\bm{Av}$, the \textit{null space} consists of all inputs for which $\bm{Av}=\bm 0$. We could generalize those terms into ``range'' and ``kernel'':
\end{remark}
\begin{definition}[range]
For a linear transformation $L:V\mapsto W$, the range (or image) of $L$ refers to the set of all outputs $T(\bm v)$, which is denoted as:
\[
\Range(L)=\{L(\bm x):x\in\bm V\}
\]
Sometimes we also use notation $\im(L)$ to express the same thing.
\end{definition}
\emph{The range corresponds to the column space}. If $L(\bm x)=\bm{Ax}$, we have $\Range(L)=\mathcal{C}(\bm A)$.
\begin{definition}[kernel]
The kernel of $L$ refers to the set of all inputs for which $L(\bm v)=\bm 0$, which is denoted as:
\[
\ker(L)=\{\bm x:L(\bm x)=\bm 0\}
\]
\end{definition}
\emph{Kernel corresponds to the null space}. If $L(\bm x)=\bm{Ax}$, we have $\ker(L)=N(\bm A)$.
\begin{remark}
For linear transformation $L:\bm V\mapsto \bm W$, where $L(\bm x)=\bm{Ax}$. We have two rules:
\[
L(\cdot):\left\{
\begin{aligned}
N(\bm A)&\mapsto \{\bm 0\}\\
\bm V&\mapsto \col(\bm A)
\end{aligned}\right.
\]
\end{remark}
\subsection{Example: differentiation}
\emph{Key idea of this section:}
\begin{quotation}
{\bfseries\textit{Suppose we know $L(\bm v_1),\dots,L(\bm v_n)$ for the basis vectors $\bm v_1,\dots,\bm v_n$,Then the linearity property produces $L(\bm v)$ for every other input vector $\bm v$}}
\end{quotation}
\emph{Reason:} Every $\bm v$ has a unique combination $c_1\bm v_1+\dots+c_n\bm v_n$ of the basis vector $\bm v_i$. Suppose $L$ is a linear transformation, then $L(\bm v)$ must be the \emph{same combination} $c_1L(\bm v_1)+\dots+c_nL(v_n)$ of the \emph{known outputs} $L(\bm v_i)$.

\paragraph{Derivative is a linear transformation}
The derivative of the functions $1,x,x^2,x^3$ are $0,1,2x,3x^2$. If we consider ``\emph{taking the derivative}'' as a transformation, whose inputs and outputs are functions, then we claim that the \emph{derivative transformation} is \emph{linear}:
\[
L(\bm v)=\frac{\diff \bm v}{\diff x}\qquad\text{obeys the linearity rule}\qquad\frac{\diff}{\diff x}(c\bm v+d\bm w) = c\frac{\diff \bm v}{\diff x}+d\frac{\diff \bm w}{\diff x}
\]
If we consider $1,x,x^2,x^3$ as vectors instead of functions, we notice they form a basis for the space $\bm V:=\{\mbox{\textit{polynomials with degree$\le 3$}}\}.$ Find derivatives of these four basis tells us all derivatives in $\bm V$:
\begin{example}
Given any vector $\bm v$ in $\bm V$, it can be expressed as $\bm v=a+bx+cx^2+dx^3$. We want to find the derivative transformation output for $\bm v$:
\[
\begin{aligned}
L(\bm v)&=aL(1)+bL(x)+cL(x^2)+dL(x^3)\\
        &=a\x(0)+b\x(1)+c\x(2x)+d\x(3x^2)\\
        &=b+2cx+3dx^2
\end{aligned}
\]
Can we express this linear transformation $L$ by a matrix $\bm A$? The answer is \textit{Yes}:

The derivative transforms the space $\bm V$ of cubics to the space $\bm W$ of quadratics. The basis for $\bm V$ is $1,x,x^2,x^3$. The basis for $\bm W$ is $1,x,x^2$. It follows that \textit{The derivative matrix is 3 by 4}:
\[
\bm A:= \begin{bmatrix}
0&1&0&0\\0&0&2&0\\0&0&0&3
\end{bmatrix}=\text{\emph{matrix form of derivative $L$}}.
\]
Why do we define the derivative matrix? Because \emph{multiplying by $\bm A$ agrees with transforming by $L$}. The derivative of $\bm v=a+bx+cx^2+dx^3$ is $L(\bm v)=b+2cx+3dx^2$. The same numbers $b,2c,3d$ appear when we multiply by matrix $\bm A$:
\[
\begin{array}{ll}
\mbox{\textit{\emph{Take the derivative}}}
&
\begin{bmatrix}
0&1&0&0\\0&0&2&0\\0&0&0&3
\end{bmatrix}\begin{bmatrix}
a\\b\\c\\d
\end{bmatrix}=\begin{bmatrix}
b\\2c\\3d
\end{bmatrix}.
\end{array}
\]
What does the matrix $\begin{bmatrix}
a\\b\\c\\d
\end{bmatrix}$ and $\begin{bmatrix}
b\\2c\\3d
\end{bmatrix}$ mean? 

It is the \emph{coordinate vector} of $\bm v$ and $L(\bm v)$. If we consider $a+bx+cx^2+dx^3$ as a vector, then it's better for us to study its corresponding coordinate vector $\begin{bmatrix}
a\\b\\c\\d
\end{bmatrix}$.

Hence, taking derivative of $\bm v$ is the same as multiplying matrix $\bm A$ by its coordinate vector.
\end{example}



\subsubsection{The inverse of the derivative.}
\paragraph{The \emph{integral} is the inverse of the derivative. } That is from the Fundamental Theorem of Calculus. We review it from the perspective of linear algebra. The integral transformation $L^{-1}$ that \textit{takes the integral from 0 to x} is also linear! Applying $L^{-1}$ to $1,x,x^2$, which are $\bm w_1,\bm w_2,\bm w_3$:
\[
\mbox{\emph{Integration is $L^{-1}$}}\qquad
\int_0^x1\diff x=x,\quad\int_0^xx\diff x=\frac{1}{2}x^2,\quad\int_0^xx^2\diff x=\frac{1}{3}x^3.
\]
By linearity, the integral of $\bm w=B+Cx+Dx^2$ is $L^{-1}(\bm w)=Bx+\frac{1}{2}Cx^2+\frac{1}{3}Dx^3$. The integral of a quadratic is a cubic. The input space of $L^{-1}$ is the quadratics, the output space is the cubics. \emph{Integration takes W back to V}. Integration matrix will be 4 by 3:
\[
\begin{array}{ll}
\mbox{\emph{Take the integral}}
&
\begin{bmatrix}
0&0&0\\1&0&0\\0&\frac{1}{2}&0\\0&0&\frac{1}{3}
\end{bmatrix}
\begin{bmatrix}
B\\C\\D
\end{bmatrix}=\begin{bmatrix}
0\\B\\\frac{1}{2}C\\\frac{1}{3}D
\end{bmatrix}.
\end{array}
\]
If our input is $\bm w=B+Cx+Dx^2$, our output integral is $0+Bx+\frac{1}{2}Cx^2+\frac{1}{3}Dx^3$.
\paragraph{The derivative and the integration are essentially matrix multiplication}
We have the corresponding derivative and integration matrix:
\[
\bm A=\begin{bmatrix}
0&1&0&0\\0&0&2&0\\0&0&0&3
\end{bmatrix}\qquad\bm A^{-1}=\begin{bmatrix}
0&0&0\\1&0&0\\0&\frac{1}{2}&0\\0&0&\frac{1}{3}
\end{bmatrix}
\]
I want to call this matrix $\bm A^{-1}$, though rectangular matrices don't have inverses. Note that $\bm A^{-1}$ is the \emph{right inverse} of matrix $\bm A$! (Do you remember the definition that shown in mid-term?)
\[
\begin{array}{lll}
\bm A\bm A^{-1}=\begin{bmatrix}
1&0&0\\0&1&0\\0&0&1
\end{bmatrix}
&
\mbox{but}
&
\bm A^{-1}\bm A=\begin{bmatrix}
0&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1
\end{bmatrix}.
\end{array}
\]
This is reasonable. If you integrate a function and then differentiate, you get back to the start. Hence $\bm A\bm A^{-1}=\bm I$. But if you differentiate before integrating, the constant term is lost.

\paragraph{The integral of the derivative of 1 is zero}
\[
L^{-1}L(1)=\text{integral of zero function}=0.
\]
\paragraph{Summary:}
In this example, we want to take the derivative. Then we let $\bm V$ be a vector space of polynomials with degree $\le 3$. Its basis is given by $E=\{1,x,x^2,x^3\}$. Any $v\in\bm V$ there is a unique linear combination of the basis vectors that equals to $v$:
\[
v=a+bx+cx^2+dx^3
\]
We write the coordinate vector of $v$ w.r.t. to $E$:
\[
[v]_{E}=\begin{bmatrix}
a\\b\\c\\d
\end{bmatrix}
\]
Then we postmultiply $\bm A$ by $[v]_{E}$ to get the corresponding coordinate vector of output space:
\[
[L(v)]_{F}=\bm A[v]_{E}
\]
where $F=\{1,x,x^2\}$.

Here we give the formal definition for the coordinate vector:
\begin{definition}[coordinate vector]
Let $\bm V$ be a vector space of dimension $n$ and let $B=\{v_1,v_2,\dots,v_n\}$ be an \emph{ordered} basis for $\bm V$. Then for any $v\in\bm V$ there is a unique linear combination of the basis vectors such that
\[
v=\alpha_1v_1+\alpha_2v_2+\dots+\alpha_nv_n
\]
where $\alpha_1,\dots,\alpha_n$ are scalars.

The \emph{coordinate vector} of $v$ w.r.t. to $B$ is defined by
\[
[v]_{B}=\begin{bmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{bmatrix}
\]
Hence, vector $v$ could be expressed as:
$
v=\begin{bmatrix}
v_1&v_2&\dots&v_n
\end{bmatrix}\x [v]_{B}.
$
\end{definition}
More specifically, the linear transformation of vectors is essentially the matrix multiplication of the corresponding coordinate vectors:
\begin{theorem}
Let $E=\{v_1,\dots,v_n\}$ be a basis for $\bm V$; $F=\{w_1,\dots,w_m\}$ be a basis for $\bm W$. Given linear transformation $L:\bm V\mapsto \bm W$, for any vector $v\in\bm V$, there exists $m\x n$ matrix $\bm A$ such that
\[
[L(v)]_F=\bm A[v]_E
\]
\end{theorem}
If we let $\bm W=\bm V$, then we obtain a more commonly useful corollary:
\begin{corollary}\label{matrix_representation}
Given linear transformation $L:\bm V\mapsto \bm V$. We set $E = \{\alpha_1,\dots,\alpha_n\}$ to be the basis of $\bm V$. Then given any vector $v$, there exists $n\x n$ matrix $\bm A$ such that
\[
[L(v)]_{E}=\bm A[v]_{E}
\]
\end{corollary}
\subsection{Basis Change}
\paragraph{Basis Change is essentially matrix multiplication}
Suppose $L:\bm V\mapsto \bm V$. $E=\{v_1,\dots,v_n\}$ is a basis for $\bm V$, $F=\{u_1,\dots,u_n\}$ is another basis for $\bm V$. Then vector $u_1,\dots,u_n$ could be expressed by vectors $v_1,\dots,v_n$. So we set
\[
\begin{aligned}
u_1&=s_{11}v_1+s_{12}v_2+\dots+s_{1n}v_n,\\
u_2&=s_{21}v_1+s_{22}v_2+\dots+s_{2n}v_n,\\
\dots\\
u_n&=s_{n1}v_1+s_{n2}v_2+\dots+s_{nn}v_n.
\end{aligned}
\]
We could write this system into matrix form:
\[
(u_1,\dots,u_n)=(v_1,\dots,v_n)\begin{pmatrix}
s_{11}&s_{12}&\dots&s_{1n}\\
s_{21}&s_{22}&\dots&s_{2n}\\
\vdots&\vdots&\dots&\vdots\\
s_{n1}&s_{n2}&\dots&s_{nn}
\end{pmatrix}.
\]
We set $\bm S=(s_{ij})$. Hence we obtain:
\begin{equation}\label{basis_change_formula}
(u_1,\dots,u_n)=(v_1,\dots,v_n)\bm S.
\end{equation}
You should \emph{prove it by yourself} that $\bm S$ is invertible. Hence we have:
\begin{equation}\label{basis_change_formula_inverse}
(u_1,\dots,u_n)\bm S^{-1}=(v_1,\dots,v_n).
\end{equation}
\paragraph{We can express linear transformation in terms of different basis}
Given any vector $x\in\bm V$, we want to study the relationship between $L(x)$ and $[x]_{F}$:
\begin{equation}
\begin{aligned}
L(x)  	&=	\begin{bmatrix}v_1&v_2&\dots&v_n\end{bmatrix}\x [L(x)]_{E}\\
	    	&= \begin{bmatrix}v_1&v_2&\dots&v_n\end{bmatrix}\x (\bm A[x]_{E})\qquad\text{$\leftarrow$ due to corollary (\ref{matrix_representation})}
	    	\\
	    	&= \begin{bmatrix}u_1&u_2&\dots&u_n\end{bmatrix}\bm S^{-1}\x (\bm A[x]_{E})\\
\end{aligned}\label{Eq:5:3}
\end{equation}
\begin{itemize}
\item
We claim that $[x]_{E}=\bm S[x]_{F}$:

For any vector $x\in\bm V$, we obtain:
\[
\begin{aligned}
x &=\begin{bmatrix}v_1&v_2&\dots&v_n\end{bmatrix}\x [x]_{E}\\
&=\begin{bmatrix}u_1&u_2&\dots&u_n\end{bmatrix}\x [x]_{F}\\
&= \begin{bmatrix}v_1&v_2&\dots&v_n\end{bmatrix}\x \bm S[x]_{F}
\end{aligned}
\]
Hence $[x]_{E}=\bm S[x]_{F}$.
\end{itemize}

Substituting $[x]_{E}=\bm S[x]_{F}$ into Eq.(\ref{Eq:5:3}), we obtain:
\[
L(x)=\begin{bmatrix}u_1&u_2&\dots&u_n\end{bmatrix}\bm S^{-1}\bm A\bm S[x]_{F}
\]
What do the following process mean? We know that given basis $E=\{v_1,\dots,v_n\}$, performing linear transformation on any vector $x$ is just the same as matrix multiplication:
\[
L(x)=\begin{bmatrix}v_1&v_2&\dots&v_n\end{bmatrix}\x \bm A[x]_{E}
\]

In summary, 
\begin{enumerate}
\item
The linear transformation is essentially postmultiplying matrix for the coordiante vector:
\[
\begin{array}{lll}
x =\begin{bmatrix}v_1&v_2&\dots&v_n\end{bmatrix}\x [x]_{E}
&
\implies
&
L(x)=\begin{bmatrix}v_1&v_2&\dots&v_n\end{bmatrix}\x \bm A[x]_{E}
\end{array}
\]
\item
If we change another basis $F=\{u_1,\dots,u_n\}$, we must change $\bm A$ into $\bm S^{-1}\bm A\bm S$:
\[
\begin{array}{lll}
x =\begin{bmatrix}u_1&u_2&\dots&u_n\end{bmatrix}\x [x]_{F}
&
\implies
&
L(x)=\begin{bmatrix}u_1&u_2&\dots&u_n\end{bmatrix}\x \bm S^{-1}\bm A\bm S[x]_{F}
\end{array}
\]
\end{enumerate}
It suffices to define $\bm B:=\bm S^{-1}\bm A\bm S$, The matrix $\bm B$ is said to be \emph{similar} to $\bm A$.
\begin{definition}[Similar]
Let $\bm A,\bm B$ be $n\x n$ matrix. If there exists invertible $n\x n$ matrix $\bm S$ such that $\bm B=\bm S^{-1}\bm A\bm S$, then we say that $\bm A$ is \emph{similar} to $\bm B$.
\end{definition}
\subsection{Determinant}
The determinat of a \emph{square matrix} is a single number, which contains many amazing amount of information about the matrix. It has four major uses:
\paragraph{The determinant is zero if and only if the matrix has no inverse}
\paragraph{It can be used to calculate the area or volumn of a box} $|\det(\bm A)|$ is the volume of the parallelepiped $\mathcal{P}=\{y=\sum_{i=1}^m\alpha_i\bm a_i\mid \alpha_i\in[0,1]\}$:
\begin{figure}[H]
\centering
\includegraphics[width=6cm]{week4/F_1.png}
\caption{The parallelepiped $\mathcal{P}=\{y=\sum_{i=1}^3\alpha_i\bm a_i\mid \alpha_i\in[0,1]\}$, where $r_1,r_2,r_3$ are $\bm a_1,\bm a_2,\bm a_3$ on $\mathbb{R}^3$}
\end{figure}
\paragraph{The product of all the pivots $=(\pm 1)\x$the determinant}

For a 2 by 2 matrix $\bm A=\begin{bmatrix}
a&b\\c&d
\end{bmatrix}$, the pivots are $a$ and $d-(\frac{c}{a})b$. The product of pivots is the determinant:
\[
\begin{array}{lll}
\mbox{\emph{Product of pivots}}
&
a(d-\frac{c}{a}b)=ad-bc
&
\mbox{\emph{which is} $\det\bm A$}
\end{array}
\]
\paragraph{Compute determinants to find $\bm A^{-1}$ and $\bm A^{-1}\bm b$} (Cramer's Rule).

\subsubsection{The properties of the Determinant}
We don't intend to define the determinant directly by its formulas. It's better to start with its properties. These properties are simple, but they prepare for the formulas.
\begin{remark}
Brackets for the matrix, straight bars for its determinant. For example,
\[
\text{The determinant of }\begin{bmatrix}
a&b\\c&d
\end{bmatrix} \text{ is } \begin{vmatrix}
a&b\\c&d\end{vmatrix}=ad-bc
\]
The determinant is written in two ways, det $\bm A$ or $|\bm A|$.
\end{remark}

We will introduce three basic properties, then we will show how properties $1-3$ derive other properties.
\begin{enumerate}
\item
\emph{The determinant of the $\bm n$ by $\bm n$ identity matrix is 1.}
\begin{gather*}
\begin{vmatrix}1&0\\0&1\end{vmatrix}=1\qquad\text{and}\qquad\begin{vmatrix}
1&&\\&\ddots&\\&&1\end{vmatrix}=1.
\end{gather*}
\item
\emph{The determianant changes sign when two rows are exchanged.} (sign reversal)
\begin{gather*}
\text{Check:   }\begin{vmatrix}c&d\\a&b\end{vmatrix}=-\begin{vmatrix}a&b\\c&d\end{vmatrix}\qquad\text{(both sides equal $bc-ad$)}.
\end{gather*}
\item
\emph{The determinant is a linear function of each row separately.} (all other rows stay fixed).
\begin{gather*}
\text{\emph{multiply row 1 by any number $\bm t$}}\qquad\begin{vmatrix}ta&tb\\c&d\end{vmatrix}=t\begin{vmatrix}a&b\\c&d\end{vmatrix}
\end{gather*}
\begin{gather*}
\text{\emph{Add row 1 of $\bm A$ to row 1 of $\bm B$:}}\qquad\begin{vmatrix}a_1+a_2&b_1+b_2\\c&d\end{vmatrix}=\begin{vmatrix}a_1&b_1\\c&d\end{vmatrix}+\begin{vmatrix}a_2&b_2\\c&d\end{vmatrix}
\end{gather*}

Note that this rule \emph{deos not} mean $\det (\bm A+\bm B)=\det \bm A+\det\bm B$.

Note that this rule \emph{does not} mean $\det(t\bm A)=t\det(\bm A)$.

Actually, $\det(t\bm A)=t^n\det \bm A$. This is reasonable. Imagining that expanding a rectangle by 2, its area will increase by 4. Expand an $n-$dimensional box by $t$ and its volumn will increase by $t^n$.

Pay special attention to property $1\sim 3$. They completely determine the $\det\bm A$. We could stop here to find a formula for determinants. But before that we prefer to derive other properties that follow directly from the first three:
\item
\emph{If two rows of $\bm A$ are equal, then $\det\bm A=\bm0$.}
\begin{gather*}
\text{Check 2 by 2:   }\begin{vmatrix}a&b\\a&b\end{vmatrix}
=0.
\end{gather*}
Property 4 follows from Property 2.
\begin{proof}[Proofoutline.]
\textit{Exchange the two equal row.} The determinant $\bm D$ is supposed to change sign. But also the matrix is not changed, so we have $-\bm D=\bm D\implies\bm D=0$.
\end{proof}
\item
\emph{Adding a constant multiple of a row to another row doesn't change $\det\bm A$.}
\[
\begin{vmatrix}a+lc&b+ld\\c&d\end{vmatrix}=
\begin{vmatrix}a&b\\c&d\end{vmatrix}+\begin{vmatrix}lc&ld\\c&d\end{vmatrix}=\begin{vmatrix}a&b\\c&d\end{vmatrix}+l\begin{vmatrix}c&d\\c&d\end{vmatrix}=
\begin{vmatrix}a&b\\c&d\end{vmatrix}=\det\bm A
\]
\emph{Conclusion: }\textit{The determinant is not changed by the usual elimination step from $\bm A$ to $\bm U$}. Since every row exchange reverses the sign, we have $\det\bm A=\pm\det\bm U$.
\item
\emph{If $\bm A$ is triangular, then $\det\bm A=$ product of diagonal entries}.
\begin{gather*}
\text{\emph{Triangular}}\qquad\begin{vmatrix}a&b\\0&d\end{vmatrix}=ad\qquad\text{and also}\qquad\begin{vmatrix}a&0\\c&d\end{vmatrix}=ad
\end{gather*}
Suppose all diagonal entries of $\bm A$ are nonzero. We do Gaussian Elimination to convert $\bm A$ into diagonal matrix:
\[
\det\begin{bmatrix}
a_{11}&&&\bigzero\\&a_{22}&&\\&&\ddots&\\\bigzero&&&a_{nn}
\end{bmatrix}=a_{11}a_{22}\dots a_{nn}.
\]
Factor $a_{11}$ from the first row by property 3; then factor $a_{22}$ from the second row;$\dots\dots$. Finally the determinant is $a_{11}\x a_{22}\x a_{33}\dots\x a_{nn}\x\det\bm I=a_{11}\x a_{22}\x a_{33}\dots\x a_{nn}$.
\item 
$\det(\bm{AB})=\det(\bm A)\det(\bm B)$.
\begin{proof}\qquad\\
\begin{itemize}
\item
If $|\bm B|$ is zero, it's easy to verify that $\bm B$ is singular, then $\bm{AB}$ is singular. Thus $\det(\bm{AB})=0=\det(\bm A)\det(\bm B)$.
\item
Suppose $|\bm B|$ is not zero, and $\bm A,\bm B$ is $n\x n$ matrix. Consider the ratio $D(\bm A)=\frac{|\bm{AB}|}{|\bm B|}$. \textit{Check that this ratio has properties 1,2,3.} If so, $D(\bm A)$ has to be the determinant, say, $|\bm A|$. Thus we have $|\bm A|=\frac{|\bm{AB}|}{\bm B}$:\\\\
\emph{Property 1  }\textit{(Determinant of I)}\quad If $\bm A=\bm I$, then the ratio becomes $D(\bm A)=\frac{|\bm B|}{|\bm B|}=1.$\\\\
\emph{Property 2  }\textit{(Sign reversal)}\quad When two rows of $\bm A$ are exchanged, the same two rows of $\bm{AB}$ are also exchanged. Therefore $|\bm{AB}|$ changes sign and so does the ratio $\frac{|\bm{AB}|}{\bm B}$.\\\\
\emph{Property 3  }\textit{(Linearity)}\quad When row 1 of $\bm A$ is multiplied by $t$, so is row 1 of $\bm{AB}$. Thus the ratio is also increased by $t$. Thus we still have $|\bm A|=\frac{|\bm{AB}|}{\bm B}$.\\
If we Add row 1 of $\bm A_1$ to row 1 of $\bm A_2$. Then row 1 of $\bm A_1\bm B$ also adds to row 1 of $A_2B$. By property three, determinants add. After dividing by $|\bm B|$, the ratios add. Hence we still have $|\bm A|=\frac{|\bm{AB}|}{\bm B}$.\\\\
\textit{Conclusion: }The ratio $D(\bm A)$ has the same three properties that defines determinant, hence it equals $|\bm A|$. Hence we obtain the product rule $|\bm{AB}|=|\bm A||\bm B|$.\\
\end{itemize}
\end{proof}

Immediately here follows a corollary:
\begin{corollary}
\[\det(\bm A^{-1})=\frac{1}{\det(\bm A)}\]
\end{corollary}
\item
\emph{The transpose $\bm A\trans$ has the same determinant as $\bm A$.}
\begin{gather*}
\text{\emph{Transpose}}\qquad\begin{vmatrix}a&b\\c&d\end{vmatrix}=\begin{vmatrix}a&c\\b&d\end{vmatrix}\qquad\text{Both sides equal $ad-bc$}.
\end{gather*}
\begin{proof}
\begin{itemize}
\item
When $\bm A$ is singular, $\bm A\trans$ is also singular. Hence $|\bm A\trans|=|\bm A|=0$.
\item
Otherwise $\bm A$ has LU decomposition $\bm{PA}=\bm{LU}$. Transposing both siders gives $\bm A\trans\bm P\trans=\bm U\trans\bm L\trans$. By product rule we have
\[
\det\bm P\det\bm A=\det\bm L\det\bm U\qquad\text{and}\qquad\det\bm A\trans\det\bm P\trans=\det\bm U\trans\det\bm L\trans.
\]
\begin{itemize}
\item
Firstly, $\det\bm L=\det\bm L\trans=1$. (By property 6, they both have 1's on the diagonal).
\item
Secondly, $\det\bm U=\det\bm U\trans$. (By property 6, they have the same diagonal)
\item
Thirdly, $\det\bm P=\det\bm P\trans$. (Verify by yourself that $\bm P\trans\bm P=\bm I$. Hence $|\bm P\trans||\bm P|=1$. Since permutation matrix is obtained by exchanging rows of $\bm I$, the only possible value for determinant of permuation matrix is $\pm 1$. Hence $\bm P$ and $\bm P\trans$ must both equal to 1 or both equal to -1).
\end{itemize}
So $\bm{L,U,P}$ has the same determinants as $\bm L\trans,\bm U\trans,\bm P\trans$, Hence we have $\det\bm A=\det\bm A\trans$.
\end{itemize}
\end{proof}
\end{enumerate}












