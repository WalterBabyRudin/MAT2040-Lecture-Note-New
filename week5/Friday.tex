\chapterimage{M31_hallas}% Chapter heading image

%\chapter{Week5}

\section{Friday}\index{week5_Friday_lecture}
This lecture has two goals. The first is to see \emph{how orthogonality makes it easy to find projection matrix $\bm P$ and the projection $\Proj_{C(\bm A)}\bm b$}. \textit{Orthogonality makes the product $\bm A\trans\bm A$ a diagonal matrix}. The second goal is to \emph{show how to construct orthogonal vectors}. For matrix $\bm A=\begin{bmatrix}
a_1&a_2&\dots&a_n
\end{bmatrix}$, the columns may not be orthogonal. Then we convert $a_1,\dots,a_n$ to orthogonal vectors, which will be the columns of a new matrix $\bm Q$.
\subsection{Orthonormal basis}
The vectors $\bm q_1,\dots,\bm q_n$ are \emph{orthogonal} when their inner product $\inp{\bm q_i}{\bm q_j}$ are zero. ($i\ne j$.) With one more step--just divide each vector by its length, then the vectors become \emph{orthogonal unit vectors}. Their lengths are all 1. Then its basis is called \emph{orthonormal}.
\begin{definition}[orthonormal]
The vectors $\bm q_1,\dots,\bm q_n$ are \emph{orthonormal} if
\[
\inp{\bm q_i}{\bm q_j}=\begin{cases}
0&\text{when $i\ne j$}\qquad\text{(\emph{orthogonal} vectors)},\\
1&\text{when $i=j$}\qquad\text{(\emph{unit} vectors: $\|\bm q_i\|=1$)}.
\end{cases}
\]
Moreover, if $\bm q_1,\dots,\bm q_n$ are \emph{orthonormal}, then the basis $\{\bm q_1,\dots,q_n\}$ is called \emph{orthonormal basis}.
\end{definition}
\begin{example}
Unit vectors $\bm e_1=\begin{pmatrix}
1\\0\\\vdots\\0
\end{pmatrix},\bm e_2=\begin{pmatrix}
0\\1\\\vdots\\0
\end{pmatrix},\dots,\bm e_n=\begin{pmatrix}
0\\0\\\vdots\\1
\end{pmatrix}$ is an \textit{orthonormal basis} for $\mathbb{R}^{n}$.
\end{example}
If we want to express vector $\bm b$ as a linear combination of arbitrary basis $\{\bm q_1,\bm q_2,\dots,\bm q_n\}$, what should you do?\\
\textit{Answer:} To solve the system $\bm{Ax}=\bm b$, where $\bm A=\begin{bmatrix}
\bm q_1&\bm q_2&\dots&\bm q_n
\end{bmatrix}$.\\
What if $\{\bm q_1,\bm q_2,\dots,\bm q_n\}$ form an \emph{orthogonal} basis? How to find solution $\bm x$ s.t. 
\[
\bm b=x_1\bm q_1+x_2\bm q_2+\dots+x_n\bm q_n?
\]
\textit{Answer}: We just do the inner product of each $\bm q_i$ with $\bm b$ to get the coefficient $x_i$:
\[
\begin{aligned}
\inp{\bm q_i}{\bm b}&=x_1\inp{\bm q_i}{\bm q_1}+x_2\inp{\bm q_i}{\bm q_2}+\dots+x_n\inp{\bm q_i}{\bm q_n}\\
&=x_i\inp{\bm q_i}{\bm q_i}=x_i
\end{aligned}
\]
Since $x_i=\inp{\bm q_i}{\bm b}$, we could express $\bm b$ as:
\[
\bm b=\sum_{i=1}^{n}\inp{\bm q_i}{\bm b}\bm q_i.
\]
In this case, since $\{\bm q_1,\bm q_2,\dots,\bm q_n\}$ forms a basis, the columns of $\bm A$ must be ind. Hence $\bm A$ is invertible, then we get the solution to $\bm{Ax}=\bm b$:
\begin{equation}
\bm x=\bm A^{-1}\bm b.
\end{equation}
\begin{definition}[matrix with orthonormal columns]\qquad\\
Define $\bm Q=\begin{bmatrix}
q_1&q_2&\dots&q_n
\end{bmatrix}$. If vectors $\bm q_1,\dots,\bm q_n$ are \emph{orthonormal}, then we say $\bm Q$ is a matrix with \emph{orthonormal} columns.
\begin{remark}
Note that a matrix with \emph{orthonormal} columns is often denoted as $\bm Q$.
\end{remark}
\end{definition}
Such matrix \emph{is easy to work with} because we have:
\begin{equation}\label{eq:easy_to_work}
\bm Q\trans\bm Q=\begin{pmatrix}
\bm q_1\trans\\\bm q_2\trans\\\dots\\\bm q_n\trans
\end{pmatrix}\begin{pmatrix}
\bm q_1&\bm q_2&\dots&\bm q_n
\end{pmatrix}=\begin{pmatrix}
\bm q_1\trans\bm q_1&&\\
&\ddots&\\
&&\bm q_n\trans\bm q_n
\end{pmatrix}=\bm I.
\end{equation}
\begin{remark}
Note that a matrix with orthonormal columns $\bm Q$ is \textit{not required to be square}! Moreover, $\{\bm q_1,\dots,\bm q_n\}$ in $\bm Q$ is \textit{not required to form a basis}.
\end{remark}
\begin{definition}[orthogonal matrix]
An \emph{square} that is a \textit{matrix with orthonormal columns} is called \emph{othogonal matrix}.
\end{definition}
\begin{example}\qquad\\
If $\bm Q$ is a orthogonal matrix, while $\bm{\hat\bm Q}$ is a matrix with orthonormal columns that is \emph{not square}. Do the products $\bm Q\bm Q\trans$ and $\bm{\hat Q}\bm{\hat Q\trans}$ always be \textit{identity matrix}?\\
\textit{Answer}:
\begin{itemize}
\item
$\bm Q\bm Q\trans$ is always \textit{identity matrix}. According to equation (\ref{eq:easy_to_work}), we have $\bm Q\trans\bm Q=\bm I$.\\ Hence $\bm Q\trans$ is the left inverse of square matrix $\bm Q$.\\ Hence $\bm Q^{-1}=\bm Q\trans\implies\bm Q\bm Q\trans=\bm Q\bm Q^{-1}=\bm I$.\\
Moreover, solving $\bm{Qx}=\bm b$ is equivalent to $\bm x=\bm Q^{-1}\bm b=\bm Q\trans\bm b$, which is \textit{exactly} 
\[
\bm x=\begin{bmatrix}
\inp{\bm q_1}{\bm b}\\\inp{\bm q_2}{\bm b}\\\vdots\\\inp{\bm q_n}{\bm b}
\end{bmatrix}.
\]
\item
But the product $\bm \hat Q\bm \hat Q\trans$ will never be identity matrix. Assume $\bm \hat Q$ is a $m\times n$ matrix. ($m\ne n$.) Then it's easy to verify that $\rank(\bm \hat Q\bm \hat Q\trans)=\rank(\bm\hat Q)$.\\ Since $\bm\hat Q$ has orthonormal columns, the columns of $\bm\hat Q$ are ind. Hence $\rank(\bm\hat Q)=n$.\\ But $\rank(\bm \hat Q\bm \hat Q\trans)=\rank(\bm\hat Q)=n\ne m=\rank(\bm I_{m})$.\\
Moreover, if $\bm\hat Q$ has only one column $\bm\hat q$, then $\bm \hat Q\bm \hat Q\trans=\bm\hat q\bm\hat q\trans=\rank(1)\ne \bm I_{m}$.
\end{itemize}
\end{example}
\begin{proposition}\quad\\
If $\bm Q$ has orthonormal columns, then it \textit{leaves lengths unchanged}, in other words,
\[
\text{\emph{Same length}}\qquad\qquad
\|\bm{Qx}\|=\|\bm x\|\quad\text{for every vector $\bm x$.}
\]
Also, $\bm Q$ preserves inner products for vectors:
\[
\inp{\bm{Qx}}{\bm{Qy}}=\inp{\bm x}{\bm y}\quad\text{for every vectors $\bm x$ and $\bm y$.}
\]
\end{proposition}
\begin{proof}[Proofoutline.]
$\|\bm{Qx}\|^2=\|\bm x\|^2$ because
\[\begin{aligned}
\inp{\bm{Qx}}{\bm{Qx}}&=\bm x\trans\bm Q\trans\bm Q\bm x=\bm x\trans(\bm Q\trans\bm Q)\bm x\\
&=\bm x\trans\bm I\bm x=\bm x\trans\bm x
\end{aligned}
\]
Hence we have $\|\bm{Qx}\|=\|\bm x\|$. Just using $\bm Q\trans\bm Q=\bm I$, we can derive $\inp{\bm{Qx}}{\bm{Qy}}=\inp{\bm x}{\bm y}$.
\end{proof}
Orthogonal matrices are excellent for computations, since numbers can never grow too large when lengths of vectors are fixed.\\
In particular, if $\bm Q\in\mathbb{R}^{m\times n}$ has orthonormal columns, the least square problem is easy:\\
Although $\bm{Qx}=\bm b$ may not have a solution, but the normal equation
\[
\bm Q\trans\bm Q\bm \hat x=\bm Q\trans\bm b
\]
must have a unique solution $\bm\hat x=\bm Q\trans\bm b$. Why? Since $\bm Q\trans\bm Q=\bm I$, we derive $\bm\hat x=\bm Q\trans\bm Q\bm \hat x=\bm Q\trans\bm b$.\\
\emph{\textit{\Large Summary:}}\\
Hence the \emph{least squares solution} to $\bm{Qx}=\bm b$ is $\bm\hat x=\bm Q\trans\bm b$. In other words, $\bm Q\bm Q\trans\bm b\approx \bm b$. \emph{The projection matrix is} $\bm P=\bm Q\bm Q\trans$. Note that the projection $\Proj_{\col(\bm Q)}(\bm b)=\bm Q\bm Q\trans\bm b$ doesn't equal to $\bm b$ in general.\\
For general $\bm A$, the projection matrix is $\bm P=\bm A(\bm A\trans\bm A)^{-1}\bm A\trans$.
\subsection{Gram-Schmidt Process}
``Orthogonal is good''. So our goal for this section is: \textit{Given ind. vectors, how to make them orthonormal?}\\
We start with three ind. vectors $\bm a,\bm b,\bm c$ in $\mathbb{R}^{3}$. In order to construct orthonormal vectors, firstly we construct three \emph{orthogonal} vectors $\bm A,\bm B,\bm C$. Then we divide $\bm A,\bm B,\bm C$ by their lengths to get three \emph{orthonormal} vectors $\bm q_1=\frac{\bm A}{\|\bm A\|},\bm q_2=\frac{\bm B}{\|\bm B\|},\bm q_3=\frac{\bm C}{\|\bm C\|}.$
\newpage
Firstly we set $\bm A=\bm a$. The next vector $\bm B$ must be perpendicular to $\bm A$.\\ Look at the figure (\ref{figure_13.1}) below, We find that $\bm B=\bm b-\Proj_{\bm A}(\bm b)$. Hence
\[
\text{\emph{First Gram-Schmidt step}}\qquad
\bm B=\bm b-\frac{\inp{\bm A}{\bm b}}{\inp{\bm A}{\bm A}}\bm A.
\]
\begin{figure}[H]
\centering\includegraphics[width=5cm]{week5/gram}
\caption{Subtract projection to get $\bm B=\bm b-\Proj_{\bm A}\bm b$.}\label{figure_13.1}
\end{figure}
You can take inner product between $\bm A$ and $\bm B$ to verify that $\bm A$ and $\bm B$ are orthogonal in Figure (\ref{figure_13.1}). Note that $\bm B$ is not zero (otherwise $\bm a$ and $\bm b$ would be dep. We will show it later.)\\\\
Then we want to construct vector $\bm C$. $\bm C$ is not a linear combination of $\bm A$ and $\bm B$. (Because $\bm c$ is not a linear combination of $\bm a$ and $\bm b$.) But most likely $\bm c$ is \emph{not} perpendicular to $\bm A$ and $\bm B$. Hence we \textit{subtract $\bm c$ off its projections onto the space of $\bm A$ and $\bm B$.} to get $\bm C$:
\[
\text{\emph{Next Gram-Schmidt step}}\qquad\begin{aligned}
\bm C&=\bm c-\Proj_{\Span\{\bm A,\bm B\}}(\bm c)\\
&=\bm c-\Proj_{\bm A}(\bm c)-\Proj_{\bm B}(\bm c)\\
&=\bm c-\frac{\inp{\bm A}{\bm c}}{\inp{\bm A}{\bm A}}\bm A-\frac{\inp{\bm B}{\bm c}}{\inp{\bm B}{\bm B}}\bm B.
\end{aligned}
\]
\begin{figure}[H]\centering
\includegraphics[width=5cm]{week5/nextgram}
\end{figure}
Finally we get $\bm A,\bm B,\bm C$. Orthonormal vectors $\bm q_1,\bm q_2,\bm q_3$ are obtained by dividing their lengths (shown in Figure (\ref{Final_gram})):
\begin{figure}[H]\centering
\includegraphics[width=5cm]{week5/finalgram}
\caption{Final Gram-Schmidt step}
\label{Final_gram}
\end{figure}
Next we show an example of Gram-Schmidt step:
\begin{example}
How to construct orthonormal vectors for $\bm a=\begin{pmatrix}
1\\0\\1
\end{pmatrix},\bm b=\begin{pmatrix}
1\\0\\0
\end{pmatrix},\bm c=\begin{pmatrix}
2\\1\\0
\end{pmatrix}?$\\
\begin{itemize}
\item
Firstly we set $\bm A=\bm a=\begin{pmatrix}
1\\0\\1
\end{pmatrix}$.
\item
\[
\begin{aligned}
\bm B&=\bm b-\Proj_{\bm A}(\bm b)=\bm b-\frac{\inp{\bm A}{\bm b}}{\inp{\bm A}{\bm A}}\bm A\\
&=\begin{pmatrix}
1\\0\\0
\end{pmatrix}-\begin{pmatrix}
1\\0\\1
\end{pmatrix}\trans\begin{pmatrix}
1\\0\\0
\end{pmatrix}2^{-1}\begin{pmatrix}
1\\0\\1
\end{pmatrix}\\
&=\begin{pmatrix}
\frac{1}{2}\\0\\-\frac{1}{2}
\end{pmatrix}
\end{aligned}
\]
\item
\[
\begin{aligned}
\bm C&=\bm c-\Proj_{\bm A}(\bm c)-\Proj_{\bm B}(\bm c)=\bm c-\frac{\inp{\bm A}{\bm c}}{\inp{\bm A}{\bm A}}\bm A-\frac{\inp{\bm B}{\bm c}}{\inp{\bm B}{\bm B}}\bm B\\
&=\begin{pmatrix}
2\\1\\0
\end{pmatrix}-\begin{pmatrix}
1\\0\\1
\end{pmatrix}\trans\begin{pmatrix}
2\\1\\0
\end{pmatrix}2^{-1}\begin{pmatrix}
1\\0\\1
\end{pmatrix}-\begin{pmatrix}
\frac{1}{2}\\0\\-\frac{1}{2}
\end{pmatrix}\trans\begin{pmatrix}
2\\1\\0
\end{pmatrix}(\frac{1}{2})^{-1}\begin{pmatrix}
\frac{1}{2}\\0\\-\frac{1}{2}
\end{pmatrix}\\
&=\begin{pmatrix}
0\\1\\0
\end{pmatrix}
\end{aligned}
\]
Hence we obtain our orthonormal vectors:
\[
\bm q_1=\frac{\bm A}{\|\bm A\|}
=\begin{pmatrix}
\frac{1}{\sqrt 2}\\0\\\frac{1}{\sqrt 2}
\end{pmatrix},
,\bm q_2=\frac{\bm B}{\|\bm B\|}
=\begin{pmatrix}
\frac{1}{\sqrt 2}\\0\\-\frac{1}{\sqrt 2}
\end{pmatrix}
,\bm q_3=\frac{\bm C}{\|\bm C\|}
=\begin{pmatrix}
0\\1\\0
\end{pmatrix}
\]
\end{itemize}
And we derive the orthogonal matrix $\bm Q$:
\[
Q=\begin{pmatrix}
\frac{1}{\sqrt 2}&\frac{1}{\sqrt 2}&0\\0&0&1\\
\frac{1}{\sqrt 2}&-\frac{1}{\sqrt 2}&0
\end{pmatrix}
\]
\end{example}
But when will the Gram-Schmidt process ``fail''? Let's describle this process in general case, then we answer this question.\\
\subsubsection{Gram-Schmidt process in general case}
\emph{Input: }Ind. vectors $a_1,\dots,a_n$.\\
Firstly we want to construct orthogonal vectors $\bm A_1,\dots,\bm A_n$.\\
In step $j\in\{1,\dots,n\}$, we want to compute $a_j$ minus its projection in the space spanned by $\{\bm A_1,\bm A_2,\dots,\bm A_{j-1}\}$:
\[
\begin{aligned}
\bm A_j&=a_j-\Proj_{\Span\{\bm A_1,\bm A_2,\dots,\bm A_{j-1}\}}(a_j)\\
&=a_j-\Proj_{\bm A_1}(a_j)-\Proj_{\bm A_2}(a_j)-\dots-\Proj_{\bm A_{j-1}}(a_j)\\
&=a_j-\frac{\inp{\bm A_1}{a_j}}{\inp{\bm A_1}{\bm A_1}}\bm A_1-\frac{\inp{\bm A_2}{a_j}}{\inp{\bm A_2}{\bm A_2}}\bm A_2-\dots-\frac{\inp{\bm A_{j-1}}{a_j}}{\inp{\bm A_{j-1}}{\bm A_{j-1}}}\bm A_{j-1}
\end{aligned}
\]
After we get $\bm A_1,\dots,\bm A_n$, we can construct orthonormal vectors:
\[
\bm q_j=\frac{\bm A_j}{\|\bm A_j\|}\quad\text{for }j=1,2,\dots,n.
\]
So when do this process fail? When $\exists j$ such that $\bm A_j=\bm 0$, we cannot continue this process anymore.
\begin{proposition}
$\bm A_j\ne\bm0$ for $\forall j$ if and only if $a_1,a_2,\dots,a_n$ are ind.
\end{proposition}
\begin{proof}[Proofoutline.]
$\bm A_j=\bm0\Longleftrightarrow
a_j=\Proj_{\Span{\bm A_1,\dots,\bm A_{j-1}}}(a_j)$\\
Hence we only need to prove $\exists j$ s.t. $\bm A_j=\bm 0$ if and only if $a_1,a_2,\dots,a_n$ are dep.\\
\textit{Sufficiency. }Given $\bm A_j=\bm0$, then $a_j=\Proj_{\Span{\bm A_1,\dots,\bm A_{j-1}}}(a_j)\in\Span\{\bm A_1,\dots,\bm A_{j-1}\}$. It's easy to verify that $\Span\{\bm A_1,\dots,\bm A_{j-1}\}=\Span\{a_1,\dots,a_{j-1}\}$. Hence $a_j\in\Span\{a_1,\dots,a_{j-1}\}$. \\Hence $a_1,\dots,a_j$ are dep. Thus $a_1,\dots,a_n$ are dep.\\\\
\textit{Necessity. }Given $a_1,a_2,\dots,a_n$ are dep. Then obviously, $a_n\in\Span\{a_1,\dots,a_{n-1}\}$. It's easy to verify that $a_n=\Proj_{\Span\{a_1,\dots,a_{n-1}\}}(\bm a_n)$. Thus $a_n=\Proj_{\Span\{\bm A_1,\dots,\bm A_{n-1}\}}(\bm a_n)\implies \bm A_n=\bm0$.
\end{proof}
\subsection[The Factorization $A=QR$.]{The Factorization $\bm A=\bm{QR}$}
We know Gaussian Elimination leads to \textit{LU decomposition}; in fact, Gram-Schmidt process leads to \textit{QR factorization}. These two decomposition methods are quite important in LA, let's discuss QR factorization briefly:\\
Given a matrix $\bm A=\begin{bmatrix}
\bm a&\bm b&\bm c
\end{bmatrix}$, we finally end with a matrix $\bm Q=\begin{bmatrix}
\bm q_1&\bm q_2&\bm q_3
\end{bmatrix}$.\\ How are these two matrix related? \\
\textit{Answer:} Since the linear combination of $\bm a,\bm b,\bm c$ leads to $\bm q_1,\bm q_2,\bm q_3$ (vice versa), there must be a third matrix connecting $\bm A$ to $\bm Q$. This third matrix is the triangular $\bm R$ such taht $\bm A=\bm{QR}$.\\
In general case, $\bm a_1,\dots,\bm a_k$ are combinations of $\bm q_1,\dots,\bm q_k$ at every step.\\ (In general suppose $\bm A=\begin{bmatrix}
\bm a_1&\bm a_2&\dots&\bm a_n
\end{bmatrix}, \bm Q=\begin{bmatrix}
\bm q_1&\bm q_2&\dots&\bm q_n
\end{bmatrix}$)
\newpage
Let's discuss a specific example to show how to do factorization.
\begin{example}
Given $\bm A=\begin{bmatrix}
\bm a&\bm b&\bm c
\end{bmatrix}$, whose columns are ind. We can write $\bm A$ as:
\[
\bm A=\begin{bmatrix}
\bm q_1&\bm q_2&\bm q_3
\end{bmatrix}\begin{bmatrix}
\bm q_1\trans\bm a&\bm q_1\trans\bm b&\bm q_1\trans\bm c\\0&\bm q_2\trans\bm b&\bm q_2\trans\bm c\\0&0&\bm q_3\trans\bm c
\end{bmatrix}
\]
where $\bm q_1,\bm q_2,\bm q_3$ are \emph{orthonormal}.\\
We define $\bm R\triangleq\begin{bmatrix}
\bm q_1\trans\bm a&\bm q_1\trans\bm b&\bm q_1\trans\bm c\\0&\bm q_2\trans\bm b&\bm q_2\trans\bm c\\0&0&\bm q_3\trans\bm c
\end{bmatrix}, \bm Q\triangleq\begin{bmatrix}
\bm q_1&\bm q_2&\bm q_3
\end{bmatrix}$.\\
Hence $\bm A$ could be factorized into:
\[
\bm A=\bm{QR}
\]
where $\bm R$ is upper triangular, $\bm Q$ is a matrix with orthonormal columns.
\end{example}
We have a theorem about QR faactorization (without proof):
\begin{theorem}
Every $m\times n$ matrix $\bm A$ with ind. columns can be factorized as
\[
\bm A=\bm{QR}
\]
where $\bm Q$ is a matrix with \textit{orthonormal columns}, $\bm R$ is a upper triangular matrix (always square).
\end{theorem}
We postmultiply $\bm Q\trans$ both sides for $\bm A=\bm{QR}$ to obtain $\bm R=\bm Q\trans\bm A$. In fact, the inverse of $\bm R$ always exists.
\enlargethispage{1cm}
\begin{proof}
suppose $\bm A=\begin{bmatrix}
\bm a_1&\bm a_2&\dots&\bm a_n
\end{bmatrix}, \bm Q=\begin{bmatrix}
\bm q_1&\bm q_2&\dots&\bm q_n
\end{bmatrix}$.
 Thus we derive \[\bm R=\bm Q\trans\bm A=\begin{bmatrix}
\bm q_1\trans\bm a_1&\bm q_1\trans\bm a_2&\dots&\bm q_1\trans\bm a_n\\
0&\bm q_2\trans\bm a_2&\dots&\bm q_2\trans\bm a_n\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\dots&\bm q_n\trans\bm a_n
\end{bmatrix}
\]
For every step $j$ we have 
\[\bm A_j=\bm a_j-\Proj_{\Span\{a_1,\dots,a_{j-1}\}}( \bm a_j),\qquad \bm q_j=\frac{\bm A_j}{\|\bm A_j\|}.
\]
Since $\inp{\bm A_j}{\bm a_j}=\inp{\bm a_j}{\bm a_j}-\inp{\Proj_{\Span\{a_1,\dots,a_{j-1}\}}(\bm a_j)}{\bm a_j}=\|a_j\|^2-\|\Proj_{\Span\{a_1,\dots,a_{j-1}\}}(\bm a_j)\|^2>0$, we have $\inp{\bm q_j}{\bm a_j}=\frac{\inp{\bm A_j}{\bm a_j}}{\|\bm A_j\|}>0.$ Hence the diagonal of $\bm R$ are all positive. Hence this triangular matrix is \textit{invertible.}
\end{proof}
\begin{proposition}
If $\bm A=\bm{QR}$, then we have a simple way to solve $\bm A\trans\bm A\bm x=\bm A\trans\bm b$.
\end{proposition}
\begin{proof}[Explain:]
Since we have
\[
\begin{aligned}
\bm A\trans\bm A\bm x&=\bm R\trans\bm Q\trans\bm Q\bm R\bm x=\bm R\trans\bm R\bm x\\
\bm A\trans\bm b&=\bm R\trans\bm Q\trans\bm b
\end{aligned}
\]
it's equivalent to solve $\bm R\trans\bm R\bm x=\bm R\trans\bm Q\trans\bm b$.\\
Sicne $\bm R$ is \textit{invertible}, we solve by substitution to get
\[
\bm x=(\bm R\trans\bm R)^{-1}\bm R\trans\bm Q\trans\bm b=\bm R^{-1}\bm Q\trans\bm b.
\]
\end{proof}\newpage
\subsection{Function Space}
Sometimes we may also discuss orthonormal basis and Gram-Schmidt process on function space. There is a simple example:
\begin{example}
For subspace $\Span\{1,x,x^2\}\subset C[-1,1]$, firstly, how to define orthogonal for the basis $\{1,x,x^2\}$?\\
\textit{Pre-requisite: }Inner product.
\[
\inp{f}{g}=\int_{a}^{b}fg\diff x \text{ for $f,g\in C[a,b]$.}\qquad
\|f\|^2=\int_{a}^{b}f^2\diff x
\]
If we have defined inner product, then we can talk about \textit{orthogonality} for $\{1,x,x^2\}$. It's easy to verify that
\[
\inp{1}{x}=0\quad\inp{x}{x^2}=0\quad\inp{1}{x^2}=\frac{2}{3}.
\]
If we do the Gram-Schmidt Process, we obtain:
\[
\bm A=1,\qquad\bm B=x,\qquad
\bm C=x^2-\frac{\inp{1}{x^2}}{\inp{1}{1}}1-\frac{\inp{x}{x^2}}{\inp{x}{x}}x=x^2-\frac{1}{3}
\]
$\bm A,\bm B,\bm C$ are \textit{orthogonal}. We can divide their length to obtain orthonormal basis:
\[
\begin{aligned}
\bm q_1&=\frac{\bm A}{\|\bm A\|}=\frac{1}{\sqrt{\int_{-1}^{1}1^2\diff x}}=\frac{1}{2}\qquad\\
\bm q_2&=\frac{\bm B}{\|\bm B\|}=\frac{x}{\sqrt{\int_{-1}^{1}x^2 \diff x}}=\frac{x}{2/3}=\frac{3}{2}x\qquad\\
\bm q_3&=\frac{\bm C}{\|\bm C\|}=\frac{x^2-\frac{1}{3}}{\sqrt{\int_{-1}^{1}(x^2-\frac{1}{3})^2 \diff x}}=\frac{x^2-\frac{1}{3}}{\frac{8}{45}}=\frac{45x^2-15}{8}
\end{aligned}
\]
Hence $\{\bm q_1,\bm q_2, \bm q_3\}$ is the orthonormal basis for $\Span\{1,x,x^2\}$.
\end{example}
\begin{example}
Consider the collection $\mathcal{F}$ of functions defined on $[0,2\pi]$, where
\[
\mathcal{F}:=\{1,\cos x,\sin x,\cos 2x,\sin 2x,\dots,\cos mx,\sin mx,\dots\}
\]
Using various trigonometric identities, we can show that if $f$ and $g$ are \emph{distinct}(different) functions in $\mathcal{F}$, we have $\int_{0}^{2\pi}fg\diff x=0$. For example,
\[
\inp{\sin x}{\sin 2x}=\int_{0}^{2\pi}\sin x\sin 2x\diff x=\int_{0}^{2\pi}\frac{1}{2}(\cos x-\cos 3x)\diff x=0.
\]
And moreover, if $f=g$, we have $\int_{0}^{2\pi}f^2\diff x=\pi$. For example,
\[
\inp{\sin 5x}{\sin 5x}=\int_{0}^{2\pi}\sin^2 5x\diff x=\int_{0}^{2\pi}\frac{1}{2}(1+\cos 10x)\diff x=\pi.
\]
In conclusion, the collection $\{1,\sin mx,\cos mx\}$ for $k=1,2,\dots$ are \textit{orthogonal} in $C[0,2\pi]$. Note that this set is \emph{not orthonormal}!
\end{example}
This example motivates the fourier transformation:
\subsection{Fourier Series}
The Fourier series of a function is its expansion into sines and cosines:
\[
f(x)=a_0+a_1\cos x+b_1\sin x+a_2\cos 2x+b_2\sin 2x+\dots
\]
where $f(x)\in C[0,2\pi]$. We have an orthogonal basis! But what kind of function could be expressed in this way? There is a theorem for this condition (without proof):
\begin{theorem}
If a function $f$ have the finite length in its function space $C[a,b]$, then it could be expressed as \textit{fourier series}.
\end{theorem}
But how to compute the coefficients $a_i's$ and $b_j's$? The key is orthogonality! For example, in order to get $a_1$, we just do the inner product between $f(x)$ and $\cos x$:
\begin{figure}[H]\centering
\includegraphics[width=5cm]{week5/1810645243}
\caption{Enjoy fourier series!}
\end{figure}
\[
\inp{f(x)}{\cos x}=a_1\inp{\cos x}{\cos x}+0
\implies
a_1=\frac{\inp{f(x)}{\cos x}}{\inp{\cos x}{\cos x}}=\frac{1}{\pi}\int_{0}^{2\pi}f(x)\cos x\diff x
\]
Similarly we derive 
\[
a_m=\frac{1}{\pi}\int_{0}^{2\pi}f(x)\cos mx\diff x
\qquad
b_m=\frac{1}{\pi}\int_{0}^{2\pi}f(x)\sin mx\diff x.
\]