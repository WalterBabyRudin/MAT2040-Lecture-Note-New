
\section{Thursday}\index{week7_Thursday_lecture}
\subsection{Review}
\begin{itemize}
\item
\emph{eigenvalue and eigenvectors}:
If for square matrix $\bm A$ we have
\[
\bm{Ax}=\lambda\bm x
\]
where $\bm x\ne\bm 0$, then we say $\lambda$ is the \textit{eigenvalue}, $\bm x$ is the \textit{eigenvector} corresponding to $\lambda$.
\item
\emph{How to compute eigenvalues and eigenvectors?}
To solve the eigenvalue problem for an $n$ by $n$ matrix, you should follow these steps:
\begin{itemize}
\item
\textit{Compute the determinant of $\lambda\bm I-\bm A$.} The determinant is a polynomial in $\lambda$ of degree $n$.
\item
\textit{Find the roots of this polynomial}, by solving $\det(\lambda\bm I-\bm A)=0$. The $n$ roots are the $n$ eigenvalues of $\bm A$. They make $\bm A-\lambda\bm I$ singular.
\item
For each eigenvalue $\lambda$, \textit{Solve $(\lambda\bm I-\bm A)\bm x=\bm 0$ to find an eigenvector $\bm x$.}
\end{itemize}
\end{itemize}
\subsection{Similarity and eigenvalues}
Which two matrices have the same eigenvalues? The similar matrices have the same eigenvalues:
\begin{definition}[Similar]
If there exists a \textit{nonsingular} matrix $\bm S$ such that
\[
\bm B=\bm S^{-1}\bm A\bm S,
\]
then we say $\bm A$ is \emph{similar} to $\bm B$.
\end{definition}
\begin{proposition}\label{proposition_15.1}
Let $\bm A$ and $\bm B$ be $n\times n$ matrices. If $\bm B$ is \textit{similar} to $\bm A$, then $\bm A$ and $\bm B$ have the same eigenvalues.
\end{proposition}
\textit{Proofidea.} Since eigenvalues are the roots of the \textit{characteristic polynomial}, so it suffices to prove these two polynomials are the same.
\begin{proof}
The \textit{characteristic polynomial} for $\bm B$ is given by
\begin{align*}
P_{\bm B}(\lambda)&=\det(\lambda\bm I-\bm B)\\
&=\det(\lambda\bm I-\bm S^{-1}\bm A\bm S)=\det(\bm S^{-1}\lambda\bm I\bm S-\bm S^{-1}\bm A\bm S)\\
&=\det(\bm S^{-1}(\lambda\bm I-\bm A)\bm S)\\
&=\det(S^{-1})\det(\lambda\bm I-\bm A)\det(\bm S)
\end{align*}
Since $\det(\bm S^{-1})\det(\bm S)=1$, we obtain:
\begin{align*}
P_{\bm B}(\lambda)&=\det(\lambda\bm I-\bm A)\\
&=P_{\bm A}(\lambda).
\end{align*}
Since they have the same \textit{characteristic polynomial}, the roots for \textit{characteristic polynomials} of $\bm A$ and $\bm B$ must be same. Hence they have the same eigenvalues.
\end{proof}
\begin{remark}
What is invarient? In other words, what is not changed during matrix transformation?
\begin{itemize}
\item
\emph{Rank} is invarient under \textit{row transformation}.
\item
\emph{Eigenvalues} is invarient undet \textit{similar transformation}.
\item
Unluckily, similar matrices usually don't have the same eigenvectors. It's easy to raise a counterexample.
\end{itemize}
\end{remark}
By using eigenvalues, we have a new proof for $\det(\bm S^{-1})=\frac{1}{\det(\bm S)}$.
\begin{proof}
Suppose $\det(\bm S)=\lambda_1\lambda_2\dots\lambda_n$, where $\lambda_i$'s are eigenvalues of $\bm S$.\\
Then there exists $\bm x_i$ such that
\[
\bm S\bm x_i=\lambda_i\bm x_i
\]
for $i=1,\dots,n$.\\
Since $\bm S$ is invertible, all $\lambda_i$'s are nonzero, and we obain:
\[
\bm x_i=\lambda_i\bm S^{-1}\bm x_i
\implies
\frac{1}{\lambda_i}\bm x_i=\bm S^{-1}\bm x_i
\]
Or equivalently, $\bm S^{-1}\bm x_i=\frac{1}{\lambda_i}\bm x_i$. $\frac{1}{\lambda_i}$'s are eigenvalues of $\bm S^{-1}$.\\ Since $S^{-1}$ is $n\times n$ matrix, $\frac{1}{\lambda_i}$'s ($i=1,\dots,n$) are the only eigenvalues of $\bm S^{-1}$.
\\ Hence the determinant of $\bm S^{-1}$ is the product of eigenvalues:
\[
\det(\bm S^{-1})=\frac{1}{\lambda_1}\frac{1}{\lambda_2}\dots\frac{1}{\lambda_n}=\frac{1}{\det(\bm S)}.
\]
\end{proof}
We can also use eigenvalue to proof the statement below:
\begin{proposition}
$\bm A$ is singular if and only if $\det(\bm A)=0.$
\end{proposition}
\begin{proof}
Suppose $\det(\bm A)=\lambda_1\lambda_2\dots\lambda_n$, where $\lambda_i$'s are eigenvalues of $\bm A$.\\
Thus
\[
\det(\bm A)=0\Longleftrightarrow
\exists\lambda_i=0\Longleftrightarrow
\exists\text{ nonzero }\bm x\text{ s.t. }\bm A\bm x=\lambda_i\bm x=0\bm x=\bm 0.
\]
Equivalently, $\bm A$ is singular.
\end{proof}
\newpage
\subsection{Diagonalization}
Proposition (\ref{proposition_15.1}) says if $\bm A$ is similar to $
\bm B$, then they have the same eigenvalues.
\begin{itemize}
\item
Q1:
\textit{What about the reverse direction?}
\item
What's the simplest form of matrix to have eigenvalues $\lambda_1,\lambda_2,\dots,\lambda_n$?\\ We can answer this question immediately. The matrix $\begin{pmatrix}
\lambda_1&&\\&\ddots&\\&&\lambda_n
\end{pmatrix}$ has the simplest form. And we often write this matrix as $\diag(\lambda_1,\dots,\lambda_n)$.\\
Q2: What we want to ask is that \textit{if $\bm A$ has eigenvalues $\lambda_1,\dots,\lambda_n$, then $\bm A$ and $\diag(\lambda_1,\dots,\lambda_n)$ have the same eigenbalues. Are they similar?}
\end{itemize}
\begin{remark}
Why the matrix $\diag(\lambda_1,\dots,\lambda_n)$ has eigenvalues $\lambda_1,\dots,\lambda_n$?\\
\textit{Answer: }Let's explain it with $n=2:$
\[
\begin{pmatrix}
\lambda_1&\\&\lambda_2
\end{pmatrix}\begin{pmatrix}
1\\0
\end{pmatrix}=\begin{pmatrix}
\lambda_1\\0
\end{pmatrix}=\lambda_1\begin{pmatrix}
1\\0
\end{pmatrix}\qquad
\begin{pmatrix}
\lambda_1&\\&\lambda_2
\end{pmatrix}\begin{pmatrix}
0\\1
\end{pmatrix}=\begin{pmatrix}
0\\\lambda_2
\end{pmatrix}=\lambda_2\begin{pmatrix}
0\\1
\end{pmatrix}
\]
General $n$ is also easy to verify.
\end{remark}
The answer to question 1 and 2 are both No! Let's raise a counterexample to explain it:\\
\begin{example}\qquad\\
If $\bm A=\begin{bmatrix}
0&1\\0&0
\end{bmatrix}$, then $P_{\bm A}(\lambda)=\det(\lambda\bm I-\bm A)=\begin{vmatrix}\lambda&-1\\0&\lambda\end{vmatrix}
$.
Hence its eigenvalues are $\lambda_1=\lambda_2=0$.\\
And $\bm A$ and $\bm D=\diag(0,0)$ have the same eigenvalues. Are they similar?\\
We assume they are similar, which means there exists invertible matrix $\bm S$ such that
\[
\bm A=\bm S^{-1}\bm D\bm S=\bm S^{-1}\begin{pmatrix}
0&0\\0&0
\end{pmatrix}\bm S=\bm 0
\]
which leads to a contradiction!
So $\bm A$ and $\bm D=\diag(\lambda_1,\lambda_2)$ are not similar.
\end{example}
Suppose $\bm A$ has eigenvalues $\lambda_1,\dots,\lambda_n$, but $\bm A$ and $\diag(\lambda_1,\dots,\lambda_n)$ may not be similar! But which matrix is similar to its diagonal matrix $\diag(\lambda_1,\dots,\lambda_n)$?
\begin{definition}[Diagonalizable]
An $n\times n$ matrix $\bm A$ is \emph{diagonalizable} if $\bm A$ is similar to a \textit{diagonal matrix}, that is to say,
$\exists$ nonsingular matrix $\bm S$ and diagonal matrix $\bm D$ such that
\[
\bm S^{-1}\bm A\bm S=\bm D
\]
We say $\bm S$ \textit{diagonalize} $\bm A$.
\end{definition}
You should remember the remarks below, they are very important. (We will show the proof for the remarks later.)
\begin{remark}
\begin{enumerate}
\item
If $\bm A$ is diagonalizable, then the column vectors of the diagonalizing matrix $\bm S$
are eigenvectors of $\bm A$ and the diagonal elements of $\bm D$ are the corresponding
eigenvalues of $\bm A$.
\item
The diagonalizing matrix $\bm S$ is not unique.
\item
If $\bm A$ is $n\x n$ and A has $n$ distinct eigenvalues, then $\bm A$ is diagonalizable. If the
eigenvalues are not distinct, then $\bm A$ may or may not be diagonalizable depending
on whether $\bm A$ has $n$ linearly independent eigenvectors.
\end{enumerate}
\end{remark}
Why is diagonalizable good?
\begin{theorem}[Diagonalization]\qquad\\
An $n\times n$ matrix $\bm A$ is \textit{diagonalizable} iff $\bm A$ has $n$ ind. eigenvectors.
\end{theorem}
\begin{proof}\qquad\\
\textit{Necessity.} Suppose $\bm A$ has $n$ ind. eigenvectors $\bm x_i$ for $i=1,\dots,n.$ And we assume $\exists \lambda_i$ such that
\[
\bm{A}\bm x_{i}=\lambda_i\bm x_{i}\text{  for }i=1,\dots,n.
\]
We multiply $\bm A$ with $\bm S=\begin{bmatrix}
\bm x_1&\bm x_2&\dots&\bm x_n
\end{bmatrix}$. The first column of $\bm{AS}$ is $\bm A\bm x_1$, that is $\lambda_1\bm x_1$. Then we obtain:
\[
\text{\emph{$\bm A$ times $\bm S$}}\qquad\qquad
\bm{AS}=\bm A\begin{bmatrix}
\bm x_1&\bm x_2&\dots&\bm x_n
\end{bmatrix}=\begin{bmatrix}
\lambda_1\bm x_1&\lambda_2\bm x_2&\dots&\lambda_n\bm x_n
\end{bmatrix}.
\]
The trick is to split this matrix $\bm{AS}$ into $\bm S$ times $\bm D$:
\[
\text{\emph{$\bm S$ times $\bm D$}}\qquad\qquad
\begin{bmatrix}
\lambda_1\bm x_1&\lambda_2\bm x_2&\dots&\lambda_n\bm x_n
\end{bmatrix}=\begin{bmatrix}
\bm x_1&\bm x_2&\dots&\bm x_n
\end{bmatrix}\begin{bmatrix}
\lambda_1&&\\&\ddots&\\&&\lambda_n
\end{bmatrix}=\bm{SD}.
\]
Hence we obtain $\bm{AS}=\bm{SD}$. Since $\bm x_i$'s are ind, there exists the inverse $\bm S^{-1}$.\\
So $\bm D=\bm S^{-1}\bm A\bm S$.\\
\textit{Sufficiency.} If $\bm A$ is diagonalizable, then there exists $\bm S$ and $\bm D$ such that
\[
\bm D=\bm S^{-1}\bm A\bm S
\]
where $\bm S$ is nonsingular. And we assume $\bm D=\diag(\lambda_1,\dots,\lambda_n)$.\\
Suppose $\bm S=\begin{bmatrix}
\bm x_1&\bm x_2&\dots&\bm x_n
\end{bmatrix}$, where $\bm x_i$'s are ind.\\
Then from equation $\bm D=\bm S^{-1}\bm A\bm S$ we obtain $\bm{AS}=\bm{SD}\implies \bm A\bm x_i=\lambda_i\bm x_i$ for $i=1,2,\dots,n$.\\
Hence $\lambda_i$'s are eigenvalues and $\bm x_i$'s are ind. eigenvectors of $\bm A$.
\end{proof}
For $n\times n$ matrix $\bm A$ which is \textit{diagonalizable}, if its eigenvectors $\{\bm x_1,\dots,\bm x_n\}$ form a basis, then for any $\bm y\in\mathbb{R}^{n}$, there exists $(c_1,c_2,\dots,c_n)$ such that
\[
\bm y=c_1\bm x_1+c_2\bm x_2+\dots+c_n\bm x_n
\]
If we consider matrix $\bm A$ as representation of linear transformation, we obtain
\begin{align*}
\bm{Ay}&=c_1\bm A\bm x_1+\dots+c_n\bm A\bm x_n\\
&=c_1\lambda_1\bm x_1+\dots+c_n\lambda_n\bm x_n
\end{align*}
So if we transform $\bm y$ into $\bm{Ay}$, it's equivalent to transform the coefficient $(c_1,\dots,c_n)$ into $(c_1\lambda_1,\dots,c_n\lambda_n)$.
\begin{gather*}
\bm y\xLongrightarrow{\bm A}\bm{Ay}
\\
(c_1,\dots,c_n)\xLongrightarrow{\bm D=\diag(\lambda_1,\dots,\lambda_n)}
(c_1\lambda_1,\dots,c_n\lambda_n)=(c_1,\dots,c_n)\begin{pmatrix}
\lambda_1&&\\&\ddots&\\&&\lambda_n
\end{pmatrix}
\end{gather*}
But is there an useful way to determine whether the eigenvectors of $\bm A$ is independent?
\newpage
\begin{theorem}
If $\lambda_1,\dots,\lambda_k$ are \textit{distinct} eigenvalues of a matrix $\bm A$ with corresponding eigenvectors $\bm x_1,\dots,\bm x_k$, then $\bm x_1,\dots,\bm x_k$ are linearly independent.
\end{theorem}
\begin{proof}\begin{itemize}
\item
Let's start with $k=2$. We assume $\lambda_1\ne\lambda_2$ but $\bm x_1,\bm x_2$ are dep.\\ That is to say, $\exists(c_1,c_2)\ne\bm 0$ s.t.
\begin{equation}\label{eq_15.1}
c_1\bm x_1+c_2\bm x_2=\bm 0.
\end{equation}
If we multiply $\bm A$ both sides, we obtain
\begin{equation}\label{eq_15.2}
\bm A(c_1\bm x_1+c_2\bm x_2)=\bm 0
\implies
c_1\lambda_1\bm x_1+c_2\lambda_2\bm x_2=\bm 0.
\end{equation}
Eq(\ref{eq_15.1})$\x\lambda_2-$Eq(\ref{eq_15.2}):
\[
(c_1\lambda_2-c_1\lambda_1)\bm x=\bm 0.
\implies
c_1(\lambda_2-\lambda_1)\bm x=\bm 0.
\]
Since $\lambda_1\ne\lambda_2$,$\bm x\ne\bm 0$, we derive $c_2=0$.\\
Similarly, if we let Eq(\ref{eq_15.1})$\x\lambda_1-$Eq(\ref{eq_15.2}) to cancel $c_2$, then we get $c_1=0$.\\
Hence $(c_1,c_2)=\bm 0$ leads to contradiction!
\item
How to proof this statement for general $k$?\\
Assume there exists $(c_1,\dots,c_k)\ne\bm 0$ s.t.
\begin{equation}
c_1\bm x_1+\dots+c_k\bm x_k=\bm 0
\end{equation}
Then 
\begin{equation}
\bm A(c_1\bm x_1+\dots+c_k\bm x_k)=c_1\lambda_1+c_2\lambda_2+\dots+c_k\lambda_k\bm x_k=\bm 0.
\end{equation}
We can let Eq$(11.4)-\lambda_k\x $Eq$(11.3)$ to cancel $\bm x_k$:
\begin{equation}
c_1(\lambda_1-\lambda_k)\bm x_1+\dots+c_k(\lambda_{k-1}-\lambda_k)\bm x_{k-1}=\bm 0.
\end{equation}
We can continue this process to cancel $\bm x_{k-1},\bm x_{k-2},\dots,\bm x_2$ to get:
\[
c_1(\lambda_1-\lambda_k)\dots(\lambda_1-\lambda_2)\bm x_1=\bm 0\qquad
\text{which forces }c_1=0.
\]
Similarly every $c_i=0$ for $i=1,\dots,n$. Here is the contradiction!
\end{itemize}
\end{proof}
\begin{corollary}
If all eigenvalues of $\bm A$ are \textit{distinct}, then $\bm A$ is \textit{diagonalizable}
\end{corollary}
\subsection[Powers of $A$]{Powers of $\bm A$}
If $\bm A=\bm S^{-1}\bm D\bm S$, then $\bm A^2=(\bm S^{-1}\bm D\bm S)(\bm S^{-1}\bm D\bm S)=\bm S^{-1}\bm D^2\bm S$.\\
In general, $\bm A^k=(\bm S^{-1}\bm D\bm S)\dots(\bm S^{-1}\bm D\bm S)=\bm S^{-1}\bm D^k\bm S$.\\
We may ask if eigenvalues of $\bm A$ are $\lambda_1,\dots,\lambda_n$, then what is the eigenvalues of $\bm A^k$? The answer is intuitive, the eigenvalues of $\bm A^k$ are $\lambda_1^k,\dots,\lambda_n^k$. But you may use the wrong way to proof this statement:
\begin{proposition}
If eigenvalues of $n\x n$ matrix $\bm A$ are $\lambda_1,\dots,\lambda_n$, then eigenvalues of $\bm A^{k}$ are $\lambda_1^k,\dots,\lambda_n^k$.
\end{proposition}
\begin{proof}[Wrong proof 1:]
Assume $\bm A=\bm S^{-1}\bm D\bm S$, then $\bm A^{k}=\bm S^{-1}\bm D^{k}\bm S$. Suppose $\bm D=\diag(\lambda_1,\dots,\lambda_n)$, then $\bm D^{k}=\diag(\lambda_1^k,\dots,\lambda_n^k)$. Hence eigenvalues of $\bm A^{k}$ are $\lambda_1^k,\dots,\lambda_n^k$.\\
This proof is wrong, because $\bm A$ may not be \textit{diagonalizable}, which means $\bm A$ may not have the form $\bm A=\bm S^{-1}\bm D\bm S$.
\end{proof}
\begin{proof}[Wrong proof 2:]
If $\bm{Ax}=\lambda\bm x$, then $\bm A^2\bm x=\bm A(\bm A\bm x)=\bm A(\lambda\bm x)=\lambda(\bm{Ax})=\lambda^2\bm x$.\\
Hence for general $k$, $\bm A^k\bm x=\lambda^k\bm x$.\\
This proof only states that if $\lambda$ is the eigenvalue of $\bm A$, then $\lambda^k$ is the eigenvalues of $\bm A^k$. But it cannot derive this proposition.\\ Let's raise a counterexample: Let eigenvalues of $\bm A$ be $\lambda_1=1,\lambda_2=1,\lambda_3=2$, the eigenvalues of $\bm A^2$ be $1^2,2^2,2^2$. Then obviously, this $\bm A$ and $\bm A^2$ is a contradiction for this proof. Because $1,2$ are the eigenvalues of $\bm A$, but this proof fails to determine its multiplicity!
\end{proof}
\subsection{Nondiagonalizable Matrices}
Sometimes we face some matrices that have too few eigenvalues. (don't count with multiplicity)\\ For example, if $\bm A=\begin{bmatrix}
0&1\\0&0
\end{bmatrix}$, it's easy to verify that its eigenvalue is $\lambda=0$ and eigenvectors are of the form $\bm x=\begin{bmatrix}
c\\0
\end{bmatrix}$.\\
However, this $2\times 2$ matrix cannot be diagonalized. Why? Let's introduce a definition:\\
\begin{definition}[Eigenspace]
Suppose $\bm A$ has $k$ distinct eigenvalues $\lambda_1,\dots,\lambda_k$. Then the eigenspace for $\bm A$ is the union of all eigenvectors. Or say, the eigenspace is the union of all null space $N(\lambda_i\bm I-\bm A)$ for $i=1,\dots,k$.\\
Moreover, the null space $N(\lambda_i\bm I-\bm A)$ is called the \emph{eigenspace corresponding to eigenvalue $\lambda_i$.}
\end{definition}
Why this 2 by 2 matrix $\bm A$ cannot be diagonalizable? Because it has two repeated eigenvalues $\lambda_1=\lambda_2=0$. And its eigenspace is of dimension $1<2$. In general, if a eigenspace for a $n\x n$ matrix has dimension $k<n$, then it cannot be diagonalizable.














