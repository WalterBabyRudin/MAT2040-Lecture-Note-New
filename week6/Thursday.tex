
\section{Thursday}\index{week7_Thursday_lecture}
\subsection{Review}
\begin{itemize}
\item
\emph{Eigenvalue and eigenvectors}:
If for \emph{square} matrix $\bm A$ we have
\[
\bm{Ax}=\lambda\bm x
\]
where $\bm x\ne\bm 0$, then we say $\lambda$ is the \textit{eigenvalue}, $\bm x$ is the \textit{eigenvector} associated with $\lambda$.
\item
\emph{How to compute eigenvalues and eigenvectors?}
To solve the eigenvalue problem for matrix $\bm A\in\mathbb{R}^{n\times n}$, you should follow these steps:
\begin{itemize}
\item
\textit{Compute the characteristic polynomial of $\lambda\bm I-\bm A$.} The determinant is a polynomial in $\lambda$ of degree $n$.
\item
\textit{Find the roots of this polynomial}, by solving $\det(\lambda\bm I-\bm A)=0$. The $n$ roots are the $n$ eigenvalues of $\bm A$. They make $\bm A-\lambda\bm I$ singular.
\item
For each eigenvalue $\lambda$, \textit{solve $(\lambda\bm I-\bm A)\bm x=\bm 0$ to find a corresponding eigenvector $\bm x$.}
\end{itemize}
\end{itemize}

\subsection{Similarity}
The similar matrices have the same eigenvalues:
\begin{definition}[Similar]
If there exists a \textit{nonsingular} matrix $\bm S$ such that
\[
\bm B=\bm S^{-1}\bm A\bm S,
\]
then we say $\bm A$ is \emph{similar} to $\bm B$.
\end{definition}
\begin{proposition}\label{proposition_15.1}
Let $\bm A$ and $\bm B$ be $n\times n$ matrices. If $\bm B$ is \textit{similar} to $\bm A$, then $\bm A$ and $\bm B$ have the same eigenvalues.
\end{proposition}
\textit{Proofidea.} Since eigenvalues are the roots of the \textit{characteristic polynomial}, so it suffices to prove these two polynomials are the same.
\begin{proof}
The \textit{characteristic polynomial} for $\bm B$ is given by
\begin{align*}
P_{\bm B}(\lambda)&=\det(\lambda\bm I-\bm B)\\
&=\det(\lambda\bm I-\bm S^{-1}\bm A\bm S)=\det(\bm S^{-1}\lambda\bm I\bm S-\bm S^{-1}\bm A\bm S)\\
&=\det(\bm S^{-1}(\lambda\bm I-\bm A)\bm S)\\
&=\det(S^{-1})\det(\lambda\bm I-\bm A)\det(\bm S)
\end{align*}
Since $\det(\bm S^{-1})\det(\bm S)=1$, we obtain:
\begin{align*}
P_{\bm B}(\lambda)&=\det(\lambda\bm I-\bm A)\\
&=P_{\bm A}(\lambda).
\end{align*}
Since they have the same \textit{characteristic polynomial}, the roots for \textit{characteristic polynomials} of $\bm A$ and $\bm B$ must be same. Therefore they have the same eigenvalues.
\end{proof}
\begin{remark}
What is invarient? In other words, what is not changed during matrix transformation?
\begin{itemize}
\item
\emph{Rank} is invarient under \textit{row transformation}.
\item
\emph{Eigenvalues} is invarient undet \textit{similar transformation}.
\item
Unluckily, similar matrices usually don't have the same eigenvectors. It's easy to raise a counterexample.
\end{itemize}
\end{remark}
By using eigenvalues, we have a new proof for $\det(\bm S^{-1})=\frac{1}{\det(\bm S)}$:
\begin{proof}
Suppose $\det(\bm S)=\lambda_1\lambda_2\dots\lambda_n$, where $\lambda_i$'s are eigenvalues of $\bm S$. Then there exists $\bm x_i$ such that
\[
\bm S\bm x_i=\lambda_i\bm x_i
\]
for $i=1,\dots,n$.

Since $\bm S$ is invertible and all $\lambda_i$'s are nonzero, we imply that:
\[
\bm S\bm x_i=\lambda_i\bm x_i
\implies
\bm x_i=\lambda_i\bm S^{-1}\bm x_i
\implies
\bm S^{-1}\bm x_i=\frac{1}{\lambda_i}\bm x_i
\]

Hence, $\frac{1}{\lambda_i}$'s are eigenvalues of $\bm S^{-1}$. Since $S^{-1}\in\mathbb{R}^{n\times n}$, $\frac{1}{\lambda_i}$'s ($i=1,\dots,n$) are the only eigenvalues of $\bm S^{-1}$.

Hence the determinant of $\bm S^{-1}$ is the product of its eigenvalues:
\[
\det(\bm S^{-1})=\frac{1}{\lambda_1}\frac{1}{\lambda_2}\dots\frac{1}{\lambda_n}=\frac{1}{\det(\bm S)}.
\]
\end{proof}
We can also use eigenvalue to proof the statement shown below:
\begin{proposition}
$\bm A$ is singular if and only if $\det(\bm A)=0.$
\end{proposition}
\begin{proof}
Suppose $\det(\bm A)=\lambda_1\lambda_2\dots\lambda_n$, where $\lambda_i$'s are eigenvalues of $\bm A$.\\
Thus
\[
\det(\bm A)=0\Longleftrightarrow
\exists\lambda_i=0\Longleftrightarrow
\exists\text{ nonzero }\bm x\text{ s.t. }\bm A\bm x=\lambda_i\bm x=0\bm x=\bm 0.
\]
Or equivalently, $\bm A$ is singular.
\end{proof}

\subsection{Diagonalization}
Proposition (\ref{proposition_15.1}) says if $\bm A$ is similar to $
\bm B$, then they have the same eigenvalues.
\paragraph{Question 1} What about the reverse direction?
\paragraph{Question 2} We all approve that the simplest form of a matrix to have eigenvalues $\lambda_1,\dots,\lambda_n$ is the diagonal matrix $\diag(\lambda_1,\dots,\lambda_n)$. Suppose $\bm A$ has eigenvalues $\lambda_1,\dots,\lambda_n$, is $\bm A$ similar to the diagonal matrix $\diag(\lambda_1,\dots,\lambda_n)$?

\begin{remark}
Why the matrix $\diag(\lambda_1,\dots,\lambda_n)$ has eigenvalues $\lambda_1,\dots,\lambda_n$?

\textit{Answer: }Let's explain it with $n=2:$
\[
\begin{pmatrix}
\lambda_1&\\&\lambda_2
\end{pmatrix}\begin{pmatrix}
1\\0
\end{pmatrix}=\begin{pmatrix}
\lambda_1\\0
\end{pmatrix}=\lambda_1\begin{pmatrix}
1\\0
\end{pmatrix}\qquad
\begin{pmatrix}
\lambda_1&\\&\lambda_2
\end{pmatrix}\begin{pmatrix}
0\\1
\end{pmatrix}=\begin{pmatrix}
0\\\lambda_2
\end{pmatrix}=\lambda_2\begin{pmatrix}
0\\1
\end{pmatrix}
\]
The case for general $n$ is also easy to verify.
\end{remark}

The answers to Question 1 and 2 are both \emph{No}! Let's raise a counterexample to explain it:

\begin{example}
We give a counterexample to show that two matrices with the same eigenvalues are not necessarily similar to each other; and $\bm A$ does not necessarily similar to the corresponding diagonal matrix.

Given $\bm A=\begin{bmatrix}
0&1\\0&0
\end{bmatrix}$, then $P_{\bm A}(\lambda)=\det(\lambda\bm I-\bm A)=\begin{vmatrix}\lambda&-1\\0&\lambda\end{vmatrix}
$.  Hence its eigenvalues are $\lambda_1=\lambda_2=0$.

Hence, $\bm A$ and $\bm D=\diag(0,0)$ have the same eigenvalues. Then we show that $\bm A$ and $\bm D$ are not similar:

Assume they are similar, which means there exists invertible matrix $\bm S$ such that
\[
\bm A=\bm S^{-1}\bm D\bm S=\bm S^{-1}\begin{pmatrix}
0&0\\0&0
\end{pmatrix}\bm S=\bm 0\implies\mbox{contradiction!}
\]
\end{example}
Suppose $\bm A$ has eigenvalues $\lambda_1,\dots,\lambda_n$, but $\bm A$ and $\diag(\lambda_1,\dots,\lambda_n)$ may not be similar! We are curious about what kind of matrix can be similar to a diagonal matrix:
\begin{definition}[Diagonalizable]
An $n\times n$ matrix $\bm A$ is \emph{diagonalizable} if $\bm A$ is similar to a \textit{diagonal matrix}, that is to say,
$\exists$ nonsingular matrix $\bm S$ and diagonal matrix $\bm D$ such that
\begin{equation}\label{Eq:7:6}
\bm S^{-1}\bm A\bm S=\bm D
\end{equation}
We say $\bm S$ \textit{diagonalizes} $\bm A$.
\end{definition}
\begin{remark}
Note that Eq.(\ref{Eq:7:6}) can be equivalently written as $\bm{AS}=\bm{SD}$, or in column-by-column form:
\begin{equation}
\begin{array}{ll}
\bm{A}\bm s_i=d_i\bm s_i,
&
i=1,\dots,n,
\end{array}\label{Eq:7:7}
\end{equation}
where $\bm s_i$ denotes the $i$th column of $\bm S$, $d_i$ denotes the $(i,i)$th entry of $\bm D$. The equivalent form Eq.(\ref{Eq:7:7}) also implies that every $(\bm s_i,d_i)$ must be an eigen-pair of $\bm A$. (Proposition (\ref{Pro:7:5}))
\end{remark}
\begin{proposition}\label{Pro:7:5}
Suppose that $\bm A$ is diagonalizable, then the column vectors of the diagonalizing matrix $\bm S$
are eigenvectors of $\bm A$; and the diagonal elements of $\bm D$ are the corresponding
eigenvalues of $\bm A$.
\end{proposition}
\begin{proposition}
The diagonalizing matrix $\bm S$ is not unique.
\end{proposition}
\begin{proof}
Suppose there exists a diagonalizing matrix $\bm S$, verify by yourself that $\alpha\bm S$ is also a a diagonalizing matrix for any $\alpha\ne0$.
\end{proof}





\begin{remark}
We know that the reverse of proposition (\ref{proposition_15.1}) is not true. However, if we add one more constraint that all eigenvalues of $\bm A$ are distinct, the reverse is true. We will give a proof of it later.
\begin{enumerate}
\item
If $\bm A$ is $n\x n$ and A has $n$ distinct eigenvalues, then $\bm A$ is diagonalizable. If the
eigenvalues are not distinct, then $\bm A$ may or may not be diagonalizable depending
on whether $\bm A$ has $n$ linearly independent eigenvectors.
\end{enumerate}
\end{remark}

Why is diagonalizable good?
\begin{theorem}[Diagonalization]
A $n\times n$ matrix $\bm A$ is \textit{diagonalizable} iff $\bm A$ has $n$ independent eigenvectors.
\end{theorem}
\begin{proof}
\textit{Necessity.} 
For $n$ eigen-pairs $(\lambda_i,\bm x_i)$ of $\bm A$, suppose that $\bm x_i$'s are independent.

We after-multiply $\bm A$ with $\bm S=\begin{bmatrix}
\bm x_1&\bm x_2&\dots&\bm x_n
\end{bmatrix}$. The first column of $\bm{AS}$ is $\bm A\bm x_1=\lambda_1\bm x_1$. Hence we obtain the result for the product $\bm{AS}$:
\begin{equation}\label{Eq:7:8}
\begin{array}{ll}
\emph{$\bm A$ times $\bm S$}
&
\bm{AS}=\bm A\begin{bmatrix}
\bm x_1&\bm x_2&\dots&\bm x_n
\end{bmatrix}=\begin{bmatrix}
\lambda_1\bm x_1&\lambda_2\bm x_2&\dots&\lambda_n\bm x_n
\end{bmatrix}.
\end{array}
\end{equation}
Note that the right side of Eq.(\ref{Eq:7:8}) is essentially the product $\bm{SD}$:
\[
\begin{array}{ll}
\mbox{\emph{$\bm S$ times $\bm D$}}
&
\begin{bmatrix}
\lambda_1\bm x_1&\lambda_2\bm x_2&\dots&\lambda_n\bm x_n
\end{bmatrix}=\begin{bmatrix}
\bm x_1&\bm x_2&\dots&\bm x_n
\end{bmatrix}\begin{bmatrix}
\lambda_1&&\\&\ddots&\\&&\lambda_n
\end{bmatrix}=\bm{SD}.
\end{array}
\]
Hence we obtain $\bm{AS}=\bm{SD}$. Since $\bm x_i$'s are independent, there exists the inverse $\bm S^{-1}$.

Therefore, $\bm D=\bm S^{-1}\bm A\bm S$.\\
\textit{Sufficiency.} If $\bm A$ is diagonalizable, then there exists $\bm S$ and $\bm D$ such that
\begin{equation}
\bm D=\bm S^{-1}\bm A\bm S\label{Eq:7:9}
\end{equation}
where $\bm S$ is nonsingular. Suppose $\bm D=\diag(\lambda_1,\dots,\lambda_n)$, and $\bm S=\begin{bmatrix}
\bm x_1&\bm x_2&\dots&\bm x_n
\end{bmatrix}$, where $\bm x_i$'s are independent.

The Eq.(\ref{Eq:7:9}) can be equivalently written as $\bm{AS}=\bm{SD}$, i.e., $\bm A\bm x_i=\lambda_i\bm x_i$ for $i=1,2,\dots,n$.

Hence $\bm x_i$'s are the independent eigenvectors of $\bm A$ associated with $\lambda_i$'s.
\end{proof}

\paragraph{Diagonalizable matrix is very useful}
For diagonalizable matrix $\bm A\in\mathbb{R}^{n\times n}$, it follows that its eigenvectors $\{\bm x_1,\dots,\bm x_n\}$ are independent, i.e., form a basis for $\mathbb{R}^{n}$. Then for any $\bm y\in\mathbb{R}^{n}$,  there exists $(c_1,c_2,\dots,c_n)$ such that
\[
\bm y=c_1\bm x_1+c_2\bm x_2+\dots+c_n\bm x_n
\]
If we consider matrix $\bm A$ as representation of linear transformation, we obtain
\begin{align*}
\bm{Ay}&=c_1\bm A\bm x_1+\dots+c_n\bm A\bm x_n\\
&=c_1\lambda_1\bm x_1+\dots+c_n\lambda_n\bm x_n
\end{align*}
Hence, the linear transformation from $\bm y$ into $\bm{Ay}$ is equivalent to transforming the coordinate coefficients from $(c_1,\dots,c_n)$ into $(c_1\lambda_1,\dots,c_n\lambda_n)$:
\begin{gather*}
\bm y\xLongrightarrow{\bm A}\bm{Ay}
\\
(c_1,\dots,c_n)\xLongrightarrow{\bm D=\diag(\lambda_1,\dots,\lambda_n)}
(c_1\lambda_1,\dots,c_n\lambda_n)=(c_1,\dots,c_n)\begin{pmatrix}
\lambda_1&&\\&\ddots&\\&&\lambda_n
\end{pmatrix}
\end{gather*}
We are curious about whether there is an useful way to determine whether $\bm A$ is diagonalizable.

\begin{theorem}
If $\lambda_1,\dots,\lambda_k$ are \textit{distinct} eigenvalues of a matrix $\bm A\in\mathbb{R}^{n\times n} (n\ge k)$ with the corresponding eigenvectors $\bm x_1,\dots,\bm x_k$, then $\bm x_1,\dots,\bm x_k$ are linearly independent.
\end{theorem}
\begin{proof}\begin{itemize}
\item
Let's start with the case $k=2$. Assume that $\lambda_1\ne\lambda_2$ but $\bm x_1,\bm x_2$ are dependent, i.e., $\exists(c_1,c_2)\ne\bm 0$ s.t.
\begin{equation}\label{eq_15.1}
c_1\bm x_1+c_2\bm x_2=\bm 0.
\end{equation}

Postmultiplying $\bm A$ for Eq.(\ref{eq_15.1}) both sides results in
\begin{equation}\label{eq_15.2}
\bm A(c_1\bm x_1+c_2\bm x_2)=\bm 0
\implies
c_1\lambda_1\bm x_1+c_2\lambda_2\bm x_2=\bm 0.
\end{equation}

Eq.(\ref{eq_15.1})$\x\lambda_2 - $Eq.(\ref{eq_15.2}) results in:
\[
(c_1\lambda_2-c_1\lambda_1)\bm x=\bm 0.
\implies
c_1(\lambda_2-\lambda_1)\bm x=\bm 0.
\]

Since $\lambda_1\ne\lambda_2$ and $\bm x\ne\bm 0$, we derive $c_2=0$. Similarly, if we let Eq.(\ref{eq_15.1})$\x\lambda_1-$Eq.(\ref{eq_15.2}) to cancel $c_2$, then we get $c_1=0$.

Therefore, $(c_1,c_2)=\bm 0$ leads to a contradiction!
\item
How to proof this statement for general $k$?

Assume there exists $(c_1,\dots,c_k)\ne\bm 0$ s.t.
\begin{equation}\label{Eq:7:12}
c_1\bm x_1+\dots+c_k\bm x_k=\bm 0
\end{equation}
Then we obtain two equations from Eq.(\ref{Eq:7:12}):
\begin{align}
\bm A(c_1\bm x_1+\dots+c_k\bm x_k)&=c_1\lambda_1\bm x_1+c_2\lambda_2\bm x_2+\dots+c_k\lambda_k\bm x_k=\bm 0.\label{Eq:7:13}\\
\lambda_k(c_1\bm x_1+\dots+c_k\bm x_k)&=c_1\lambda_k\bm x_1+c_2\lambda_k\bm x_2+\dots+c_k\lambda_k\bm x_k=\bm 0.\label{Eq:7:16}
\end{align}
We can let Eq.(\ref{Eq:7:13})$-$Eq.(\ref{Eq:7:16}) to cancel $\bm x_k$:
\begin{equation}\label{Eq:7:14}
c_1(\lambda_1-\lambda_k)\bm x_1+\dots+c_k(\lambda_{k-1}-\lambda_k)\bm x_{k-1}=\bm 0.
\end{equation}
By repeatedly applying the trick from (\ref{Eq:7:12}) to (\ref{Eq:7:14}), we can show that
\[
c_1(\lambda_1-\lambda_k)\dots(\lambda_1-\lambda_2)\bm x_1=\bm 0\qquad
\text{which forces }c_1=0.
\]
Similarly every $c_i=0$ for $i=1,\dots,n$. Here is the contradiction!
\end{itemize}
\end{proof}
\begin{corollary}
If all eigenvalues of $\bm A$ are \textit{distinct}, then $\bm A$ is \textit{diagonalizable}
\end{corollary}
\subsection[Powers of $A$]{Powers of $\bm A$}
\paragraph{Matrix Powers}
If $\bm A=\bm S^{-1}\bm D\bm S$, then $\bm A^2=(\bm S^{-1}\bm D\bm S)(\bm S^{-1}\bm D\bm S)=\bm S^{-1}\bm D^2\bm S$.

In general, $\bm A^k=(\bm S^{-1}\bm D\bm S)\dots(\bm S^{-1}\bm D\bm S)=\bm S^{-1}\bm D^k\bm S$.
\paragraph{Eigenvalues of matrix powers}
We may ask if eigenvalues of $\bm A$ are $\lambda_1,\dots,\lambda_n$, then what is the eigenvalues of $\bm A^k$? The answer is intuitive, the eigenvalues of $\bm A^k$ are $\lambda_1^k,\dots,\lambda_n^k$. However, you may use the wrong way to prove this statement:
\begin{proposition}\label{Prop:7:7}
If eigenvalues of $n\x n$ matrix $\bm A$ are $\lambda_1,\dots,\lambda_n$, then eigenvalues of $\bm A^{k}$ are $\lambda_1^k,\dots,\lambda_n^k$.
\end{proposition}
\begin{proof}[Wrong proof 1:]
Assume $\bm A=\bm S^{-1}\bm D\bm S$, then $\bm A^{k}=\bm S^{-1}\bm D^{k}\bm S$. Suppose $\bm D=\diag(\lambda_1,\dots,\lambda_n)$, then $\bm D^{k}=\diag(\lambda_1^k,\dots,\lambda_n^k)$. Hence eigenvalues of $\bm A^{k}$ are $\lambda_1^k,\dots,\lambda_n^k$.

This proof is wrong, because $\bm A$ may not be \textit{diagonalizable}, which means $\bm A$ may not have the form $\bm A=\bm S^{-1}\bm D\bm S$.
\end{proof}
\begin{proof}[Wrong proof 2:]
If $\bm{Ax}=\lambda\bm x$, then $\bm A^2\bm x=\bm A(\bm A\bm x)=\bm A(\lambda\bm x)=\lambda(\bm{Ax})=\lambda^2\bm x$.\\
Hence for general $k$, $\bm A^k\bm x=\lambda^k\bm x$.

This proof only states that if $\lambda$ is the eigenvalue of $\bm A$, then $\lambda^k$ is the eigenvalues of $\bm A^k$. Unfortunately, it still cannot derive this proposition. Because it does not prove that if $\lambda$ are the eigenvalues with multiplicity $m$, then $\lambda^k$ are the eigenvalues of $\bm A^k$ with multiplicity $m$.
\\ 
Let's raise a counterexample: Let eigenvalues of $\bm A$ be $\lambda_1=1,\lambda_2=1,\lambda_3=2$; the eigenvalues of $\bm A^2$ could be $1^2,2^2,2^2$.  Hence $\bm A$ has the eigenvalues $1$ with multiplicity $2$; while $\bm A^2$ has the eigenvalue $1^2$ with multiplicity $1$. So this $\bm A$ and $\bm A^2$ is a contradiction for this proof. In other words, this proof fails to determine the multiplicity of eigenvalues.
\end{proof}
\begin{remark}
The proposition(\ref{Prop:7:7}) could be proved using \emph{Jordan form}, i.e., for any matrix $\bm A$ there exists invertible matrix $\bm S$ such that $\bm A=\bm S^{-1}\bm U\bm S$, where $\bm U$ is an upper triangular matrix with diagonal entries $\lambda_1,\dots,\lambda_n$. Then $\bm A^k=\bm S^{-1}\bm U^k\bm S$, where $\bm U^k$ is an upper triangular matrix with diagonal entries $\lambda_1^k,\dots,\lambda_n^k$. Hence the eigenvalues of $\bm A^k$ are $\lambda_1^k,\dots,\lambda_n^k$.
\end{remark}

\subsection{Nondiagonalizable Matrices}
Sometimes we face some matrices that have too few eigenvalues. (don't count with multiplicity)

For example, given $\bm A=\begin{bmatrix}
0&1\\0&0
\end{bmatrix}$, it's easy to verify that its eigenvalue is $\lambda=0$ and eigenvectors are of the form $\bm x=\begin{bmatrix}
c\\0
\end{bmatrix}$.

This $2\times 2$ matrix cannot be diagonalized. Why? Let's introduce a definition first:
\begin{definition}[Eigenspace]
Suppose $\bm A\in\mathbb{R}^{n\times n}$ has $k$ distinct eigenvalues $\lambda_1,\dots,\lambda_k$. Then the eigenspace for $\bm A$ associated with $\lambda_i$ is the collection of all eigenvectors associated with the eigenvalue $\lambda_i$, i.e., the null space $N(\lambda_i\bm I-\bm A)$.
\end{definition}

Why does this 2 by 2 matrix $\bm A$ cannot be diagonalizable? Because the the dimension of its eigenspace is too small, i.e., $\mbox{eigenspace}(\bm A,\lambda=0)=1<2$. In general, if the dimension of eigenspace associated with the eigenvalue $\lambda_i$ is less than the multiplicity of this eigenvalue, then this matrix cannot be diagonalizable. We will discuss it in the next lecture.














