
%\chapter{Week2}

\section{Wednesday}\index{week2_Wednesday_lecture}
\subsection{Remarks on Gaussian Elimination}
Gaussian Elimination to compute $\bm A^{-1}$ is equivalent to solving $n$ linear systems $\bm{Ax_{i}} =e_i$, $i=1,2,\dots,n$.
\paragraph{Computing Complexity}
For each $i$ solving $\bm{Ax_i} = e_i$ takes $O(n^3)$ operations.
\begin{itemize}
\item
Hence, solving these systems one by one take $O(n^4)$ time.
\item
However, if we solve $\bm{Ax_i} = e_i$ for $i=1,2,\dots,n$ simultaneously (that means we write all $b_i$ at the right side of the Augmented matrix), by Gaussian Elimination, it only takes  $O(n^3)$ operations.
\end{itemize}
\paragraph{Large Scale Inverse Computation}
Gaussian Elimination is not a good job for large scale sparse matrix (\emph{sparse matrix} is a matrix in which most of the elements are zero. If given a $1000\times 1000$ sparse matrix, it is expensive to do Gaussian Elimination on this matrix).

Actually, for such matrix we use \textit{iterative method} to solve it.
\paragraph{Gaussian Elimination is just a sequence of matrix multiplications}
Given nonsingular matrix $\bm A$, Gaussian Elimination is really a sequence of multiplications by elementary matrices $\bm E$'s and permutation matrix $\bm P$:
\[
\bm E\cdots\bm E\bm P \bm A= \bm U,
\]
where $\bm U$ is an upper triangular matrix.

By postmultiplying $\bm U^{-1}$ we obtain 
\[
\bm U^{-1}(\bm E\ldots\bm E\bm P \bm A) = \bm I
\implies (\bm U^{-1}\bm E\ldots\bm E\bm P )\bm A = \bm I.
\]
Furthermore, we could decompose $\bm A$ as the product of a permutation matrix, a lower triangular matrix and an upper triangular matrix:
\[
\bm A = \bm P^{-1}(\bm E^{-1}\ldots\bm E^{-1})\bm U
\]
\subsection{Properties of matrix}
\begin{enumerate}
\item
If $\bm A$ is a diagonal matrix which is given by
\[
\bm A = \begin{bmatrix}
d_1&&0\\
&\vdots&\\
0&&d_n
\end{bmatrix},
\]
and $d_1d_2d_3\dots d_n \ne 0$, then $\bm A^{-1}$ exists, and
$
\bm A^{-1} = \begin{bmatrix}
d_1^{-1}&&0\\
&\vdots&\\
0&&d_n^{-1}
\end{bmatrix}.
$
\item
If $\bm D_1,\bm D_2$ are diagonal and their product exists, then we have
\[
\bm D_1\bm D_2 = \bm D_2\bm D_1
\]
\item
If $\bm A,\bm B$ are both invertible, then $\bm{AB}$ is also invertible. The inverse of product $\bm{AB}$ is 
\[
(\bm{AB})^{-1} = \bm B^{-1}\bm A^{-1}
\]
\begin{proof}[Proofoutline.]
To see why the order is reversed, firstly multiply $\bm{AB}$ with $\bm B^{-1}\bm A^{-1}$:
\[
\bm{AB}(\bm B^{-1}\bm A^{-1}) = \bm{A}(\bm B\bm B^{-1})\bm A^{-1} = \bm A \bm I\bm A^{-1} = \bm A\bm A^{-1} = \bm I
\]
Similarly, $\bm B^{-1}\bm A^{-1}$ times $\bm{AB}$ leads to the same result. Hence we draw the conclusion: Inverse come in reverse order.
\end{proof}
\item
The same reverse order applies to three or more matrix:\\
If $\bm A,\bm B,\bm C$ are nonsingular, then $(\bm{ABC})^{-1} = \bm C^{-1}\bm B^{-1}\bm A^{-1}$.
\item
It's hard to say whether $(\bm A+\bm B)$ is invertible, but we have an interesting property:
\[
\begin{array}{ll}
\mbox{When $\bm A$ is ``small'' (we will explain it later), we have }&
(\bm I-\bm A)^{-1} = \sum_{i=1}^{\infty}\bm A^{i}
\end{array}
\]
\item
\emph{A triangular matrix is invertible if and only if no diagonal entries are zero.}\\
In order to explain it, let's discuss an example:
\begin{example}\qquad\\
We want to find the inverse of a lower triangular matrix $\bm A$:
\[
\bm A = \begin{bmatrix}
1&0&0&0\\
1&1&0&0\\
1&1&1&0\\
1&1&1&1
\end{bmatrix}
\]
Thus we do Gaussian Elimination to compute solution to $\bm{Ax} = \bm I$:
\[\left[
\begin{array}{@{}cccc|cccc@{}}
1&0&0&0&1&0&0&0\\
1&1&0&0&0&1&0&0\\
1&1&1&0&0&0&1&0\\
1&1&1&1&0&0&0&1
\end{array}\right]
\implies 
\left[
\begin{array}{@{}cccc|cccc@{}}
1&0&0&0&1&0&0&0\\
0&1&0&0&-1&1&0&0\\
0&0&1&0&0&-1&1&0\\
0&0&0&1&0&0&-1&1
\end{array}\right]
\]
This result is obtained by three row operations:
\begin{enumerate}
\item
``Add $(-1)\x$ row 3 to row 4'';
\item
``Add $(-1)\x$ row 2 to row 3'';
\item
``Add $(-1)\x$ row 1 to row 2''.
\end{enumerate}
\end{example}
\begin{proof}
Only for a nonzero diagonal lower triangular matrix, we can continue the Gaussian Elimination to convert it into identity matrix.
\end{proof}

\item
Given an invertible lower triangular matrix $\bm A$, the inverse of $\bm A$ remains lower triangular.
\item
The LDU decomposition is unique for an invertible matrix. (We assume the existence of the LDU decomposition).
\begin{proof}
\begin{itemize}
\item
Assume the invertible matrix $\bm A$ could be decomposed as:
\[
\bm A = \bm L_{1}\bm D_{1}\bm U_{1} = \bm L_{2}\bm D_{2}\bm U_{2}
\]
\item
By aftermultiplying $\bm U_{1}^{-1}$ and postmultiplying $\bm L_{2}^{-1}$ for the latter equation, we obtain:
\begin{equation}
\bm L_{1}\bm D_{1}\bm U_{1} = \bm L_{2}\bm D_{2}\bm U_{2}
\implies 
\bm L_{2}^{-1}\bm L_{1}\bm D_{1} = \bm D_{2}\bm U_{2}\bm U_{1}^{-1}\label{Eq:2:1}
\end{equation}
\item
Note that $\bm L_{2}^{-1}\bm L_{1}$ remains lower triangular with unit diagonal, thus $\bm L_{2}^{-1}\bm L_{1}\bm D_{1}$ must be lower triangular matrix. Similarly, $\bm D_{2}\bm U_{2}\bm U_{1}^{-1}$ must be upper triangular matrix. Hence $\bm L_{2}^{-1}\bm L_{1}\bm D_{1}$ and $\bm D_{2}\bm U_{2}\bm U_{1}^{-1}$ must be \textit{diagonal} matrix due to equality (\ref{Eq:2:1}).
\item
Note that the diagonal of $\bm L_{2}^{-1}\bm L_{1}\bm D_{1}$ is the same as the diagonal of $\bm D_{1}$ since $\bm L_{2}^{-1}\bm L_{1}$ has unit diagonal. Hence 
\begin{equation}
\bm L_{2}^{-1}\bm L_{1}\bm D_{1} = \bm D_1.\label{Eq:2:2}
\end{equation}
Similarly, 
\begin{equation}
\bm D_{2}\bm U_{2}\bm U_{1}^{-1} = \bm D_{2}.\label{Eq:2:3}
\end{equation}
Combining (\ref{Eq:2:1}) to (\ref{Eq:2:3}), we derive $\bm D_{1} = \bm D_{2}$.
\item
Furthermore, 
\[
\bm L_{2}^{-1}\bm L_{1}\bm D_{1} = \bm D_{1}\implies
\bm L_{2}^{-1}\bm L_{1}=\bm I\implies\bm L_1 = \bm L_2
\]
Similarly, $\bm U_{1} = \bm U_{2}$.
\end{itemize}
\end{proof}
\end{enumerate}
\subsection{matrix transpose}
We introduce a new matrix, it is the \emph{transpose} of $\bm A$:
\begin{definition}[Transpose]
The \emph{transpose} of matrix $\bm A\in\mathbb{R}^{m\times n}$ is denoted as $\bm A\trans$. The columns of $\bm A\trans$ are the rows of $\bm A$, i.e., $\bm A\trans$ means that
\[
\bm A\trans=\begin{bmatrix}
a_{11}&a_{21}&\cdots&a_{m1}\\
a_{12}&a_{22}&\cdots&a_{m2}\\
\vdots&\vdots&\ddots&\vdots\\
a_{1n}&a_{2n}&\cdots&a_{mn}\\
\end{bmatrix}\in\mathbb{R}^{n\times n}
\]
\end{definition}

For example, 
\begin{itemize}
\item
given a column vector $x\in \mathbb{R}^{n}$, the transpose $x\trans = (x_1,x_2,\dots,x_n)$ is row vector.
\item
When $\bm A$ is $m\x n$ matrix, the transpose is $n\x m$:
\[
\bm A = \begin{bmatrix}
2 & 1 & 4\\0 &0 &3
\end{bmatrix}\qquad \bm A\trans = \begin{bmatrix}
2&0\\1&0\\4&3
\end{bmatrix}\qquad (\bm A\trans)\trans = \bm A
\] 
\end{itemize}
The entry in row $i$, column $j$ of $\bm A\trans$ comes from row $j$, column $i$ of the original matrix $\bm A$:
\[
\begin{array}{ll}
\mbox{\emph{Exchange rows and columns}}
&
(\bm A\trans)_{ij} = \bm A_{ji}
\end{array}
\]
The rules for transposes are very direct:
\begin{proposition}
\begin{itemize}
\item
\emph{Sum}\qquad The transpose of $\bm A+\bm B$ is $\bm A\trans + \bm B\trans$.
\item
\emph{Product}\qquad The transpose of $\bm{AB}$ is $(\bm{AB})\trans = (\bm B)\trans(\bm A)\trans.$
\end{itemize}
\end{proposition}
\newpage

\begin{proof}[Proofoutline of Product Rule.]\quad\\

\begin{itemize}
\item
We start with $(\bm{A}x)\trans = x\trans\bm A\trans$, where $x$ refers to a vector: 

\emph{$\bm{A}x$ combines the columns of $\bm A$; while $x\trans\bm A\trans$ combines the rows of $\bm A\trans$.}

Since they are the same combinations of the same vectors, we obtain $(\bm{A}x)\trans = x\trans\bm A\trans$.
\item
Now we can prove the formula $(\bm{AB})\trans = (\bm B)\trans(\bm A)\trans$, where $\bm B$ has several columns:

Assuming $\bm B = \left[
\begin{array}{c|c|c|c}
b_1&b_2&\ldots&b_k
\end{array}\right]$, then Transposing $\bm{AB} = \left[
\begin{array}{c|c|c|c}
\bm Ab_1&\bm Ab_2&\ldots&\bm Ab_k
\end{array}\right]$ gives
\[
(\bm A\bm B)\trans = \begin{bmatrix}
b_1\trans\bm A\trans\\b_2\trans\bm A\trans\\\vdots\\b_k\trans\bm A\trans
\end{bmatrix},
\]
which is actually $\bm B\trans\bm A\trans$.
\end{itemize}
\end{proof}
\subsubsection{symmetric matrix}
For a \textit{symmetric matrix}, transposing $\bm A$ into $\bm A\trans$ makes no change.
\begin{definition}[symmetric matrix]
A matrix $\bm A\in\mathbb{R}^{n\times n}$ is \emph{symmetric matrix} if we have $\bm A = \bm A\trans$. This means that $a_{ij} = a_{ji}$ for all $i,j$. We usually denote it as $\bm A\in\mathbb{S}^{n\times n}$.
\end{definition}

Choose any matrix $\bm A$ (probably rectangular), then postmultiplying $\bm A\trans$ for $\bm A$ automatically leads to a square symmetric matrix:
\begin{quotation}
\emph{The transpose of $\bm A\trans\bm A$ is $\bm A\trans(\bm A\trans)\trans$, which is $\bm A\trans\bm A$.}
\end{quotation}
The matrix $\bm A\bm A\trans$ is also symmetric. But note that $\bm A\bm A\trans$ is a different matrix from $\bm A\trans\bm A$.
\begin{remark}
For two vector $x$ and $y$,
\begin{itemize}
\item
\emph{The dot product or inner product is denoted as $x\trans y$}
\item
\emph{The rank one product or outer product is denoted as $xy\trans$}
\end{itemize}
$x\trans y$ is a number while $xy\trans$ is a matrix.
\end{remark}
We introduce a matrix that seems opposite to symmetric matrix:
\begin{definition}[Skew-symmetric]
For matrix $\bm A$, if we have $\bm A\trans = -\bm A$, then we say $\bm A$ is \emph{skew-symmetric} or \emph{anti-symmetric}.
\end{definition}
Moreover, any $n\x n$ matrix can be decomposed as the summation of a symmetric and a skew-symmetric matrix. Let's prove it in the next lecture.




